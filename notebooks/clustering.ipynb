{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, chisquare, kruskal, ks_2samp, chi2_contingency\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.cluster import MiniBatchKMeans, AffinityPropagation\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, FunctionTransformer\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "test_df = pd.read_csv(r'/kaggle/input/playground-series-s5e2/test.csv')\n",
    "train_df = pd.read_csv(r'/kaggle/input/playground-series-s5e2/train.csv')\n",
    "# train_extra_df = pd.read_csv(r'/kaggle/input/playground-series-s5e2/training_extra.csv')\n",
    "# train_df = pd.concat([train_df, train_extra_df], ignore_index=True)\n",
    "# train_df.sample(n=100_000)\n",
    "\n",
    "target = 'price'\n",
    "\n",
    "def prepare_data(df: pd.DataFrame, is_train: bool = True):\n",
    "    \"\"\"\n",
    "    Prepares the dataset for training or testing by renaming columns, handling missing values,\n",
    "    converting categorical and numerical features, and creating new features.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe (train or test).\n",
    "        is_train (bool): Indicates if the dataframe is training data (default is True).\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The processed dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the column names\n",
    "    columns = [\n",
    "        'id', 'brand', 'material', 'size', 'compartments', \n",
    "        'laptop_compartment', 'is_waterproof', 'style', 'color', \n",
    "        'weight_capacity'\n",
    "    ]\n",
    "    \n",
    "    if is_train:\n",
    "        columns.append('price')\n",
    "    \n",
    "    df.columns = columns\n",
    "    df = df.drop(columns='id')\n",
    "    \n",
    "    # Define the mapping for Size conversion\n",
    "    size_mapping = {\"Small\": 1, \"Medium\": 2, \"Large\": 3}\n",
    "    df[\"size_int\"] = df[\"size\"].map(size_mapping).fillna(0).astype(int)\n",
    "    \n",
    "    # Handle weight capacity\n",
    "    df['weight_capacity'] = df['weight_capacity'].fillna(0)\n",
    "    df['weight_capacity_int'] = df['weight_capacity'].astype(int)\n",
    "    df['weight_capacity_size'] = df['weight_capacity'] * df['size_int']\n",
    "    \n",
    "    # Convert categorical columns\n",
    "    df['compartments'] = df['compartments'].astype('category')\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    df[cat_cols] = df[cat_cols].astype('category')\n",
    "    \n",
    "    # Convert boolean columns to integer type\n",
    "    df['laptop_compartment'] = df['laptop_compartment'].cat.codes.fillna(-1).astype(int)\n",
    "    df['is_waterproof'] = df['is_waterproof'].cat.codes.fillna(-1).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply function to train and test datasets\n",
    "train_df = prepare_data(train_df, is_train=True)\n",
    "test_df = prepare_data(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = [\n",
    "    'weight_capacity', 'weight_capacity_int', 'weight_capacity_size', 'size_int', 'color', 'compartments', 'brand', 'material', 'is_waterproof'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['weight_capacity', 'weight_capacity_int', 'weight_capacity_size', 'size_int', 'is_waterproof']\n",
    "cat_cols = ['brand', 'material', 'compartments', 'color']\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[('scaler', StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[('onehot', OneHotEncoder(handle_unknown='ignore', drop=None))]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder='drop'  # drop other columns not listed\n",
    ")\n",
    "\n",
    "X = train_df[model_features].copy()\n",
    "y = train_df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MiniBatchKMeans\n",
    "model_start_time = time.time()\n",
    "kmeans_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('clusterer', MiniBatchKMeans(n_clusters=5, random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "kmeans_pipeline.fit(X)\n",
    "kmeans_labels = kmeans_pipeline.named_steps['clusterer'].labels_\n",
    "train_df['cluster'] = kmeans_labels\n",
    "print(f\"MiniBatchKMeans Training & Prediction time: {time.time() - model_start_time:.2f} seconds\")\n",
    "\n",
    "print(\"MiniBatchKMeans cluster counts:\")\n",
    "display(pd.Series(kmeans_labels).value_counts().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_clusters(cluster_labels_dict, X, y, kf, verbose=True):\n",
    "    # LightGBM parameters\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'verbose': -1,\n",
    "        'force_row_wise': True\n",
    "    }\n",
    "\n",
    "    # This will store, for each clustering method, the RMSEs per model across folds\n",
    "    all_results = {}\n",
    "    # X = train_df[['cluster'] + model_features].copy()\n",
    "    \n",
    "    # Prepare a dict to accumulate fold RMSE scores for each model\n",
    "    model_scores = {name: [] for name in cluster_labels_dict.keys()}\n",
    "\n",
    "    # K-fold cross validation\n",
    "    for fold, (train_index, valid_index) in enumerate(kf.split(train_df), start=1):\n",
    "\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "        \n",
    "        # Loop over each clustering method and its cluster labels\n",
    "        for method_name, n_clusters in cluster_labels_dict.items():\n",
    "\n",
    "            if n_clusters == 1:\n",
    "                X_train['cluster'] = 0\n",
    "                X_valid['cluster'] = 0\n",
    "            else:\n",
    "                kmeans_pipeline = Pipeline(\n",
    "                    steps=[\n",
    "                        ('preprocessing', preprocessor),\n",
    "                        ('clusterer', MiniBatchKMeans(n_clusters=n_clusters, random_state=42))\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                kmeans_pipeline.fit(X_train)\n",
    "                kmeans_labels = kmeans_pipeline.named_steps['clusterer'].labels_\n",
    "                X_train['cluster'] = kmeans_labels\n",
    "                X_valid['cluster'] = kmeans_pipeline.predict(X_valid)\n",
    "\n",
    "            y_pred_valid = y_valid.reset_index()\n",
    "            y_pred_valid.loc[:, 'y_pred'] = 0\n",
    "            y_pred_valid = y_pred_valid.set_index('index')\n",
    "    \n",
    "            # Train a separate model for each cluster\n",
    "            for c in range(n_clusters):\n",
    "                train_cluster = X_train[X_train.cluster == c].copy()\n",
    "                val_cluster = X_valid[X_valid.cluster == c].copy()\n",
    "                \n",
    "                train_data = lgb.Dataset(train_cluster[model_features], \n",
    "                                         label=y_train[X_train.cluster == c])\n",
    "    \n",
    "                fit_model = lgb.train(\n",
    "                    params,\n",
    "                    train_data,\n",
    "                    num_boost_round=100,\n",
    "                    valid_sets=[train_data],\n",
    "                )\n",
    "                \n",
    "                y_pred_valid.loc[X_valid.cluster == c, 'y_pred'] = fit_model.predict(val_cluster[model_features], num_iteration=fit_model.best_iteration)\n",
    "    \n",
    "            # Calculate RMSE for this model across all validation samples\n",
    "            rmse = np.sqrt(mean_squared_error(y_valid, y_pred_valid['y_pred']))\n",
    "            model_scores[method_name].append(rmse)\n",
    "\n",
    "        # Summarize scores for this method\n",
    "        df_scores = pd.DataFrame(model_scores)\n",
    "        all_results[method_name] = df_scores\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels_dict = {\n",
    "    \"baseline\": 1,\n",
    "    \"kmeans_2\": 2,\n",
    "    \"kmeans_3\": 3,\n",
    "    \"kmeans_4\": 4,\n",
    "    \"kmeans_5\": 5,\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cluster_cv_results = cv_clusters(cluster_labels_dict, X, y, kf, verbose=True)\n",
    "\n",
    "# Inspect results\n",
    "result_kmeans = cluster_cv_results[\"kmeans_5\"]\n",
    "print(\"KMeans per-fold RMSE:\")\n",
    "display(result_kmeans.round(3))\n",
    "\n",
    "# Summarize\n",
    "summary_kmeans = pd.DataFrame({\n",
    "    \"Mean RMSE\": result_kmeans.mean(),\n",
    "    \"Std RMSE\": result_kmeans.std()\n",
    "})\n",
    "summary_kmeans.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, chisquare, kruskal, ks_2samp, chi2_contingency\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, FunctionTransformer\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "test_df = pd.read_csv(r'/kaggle/input/playground-series-s5e2/test.csv')\n",
    "train_df = pd.read_csv(r'/kaggle/input/playground-series-s5e2/train.csv')\n",
    "train_extra_df = pd.read_csv(r'/kaggle/input/playground-series-s5e2/training_extra.csv')\n",
    "train_df = pd.concat([train_df, train_extra_df], ignore_index=True)\n",
    "# train_df.sample(n=100_000)\n",
    "\n",
    "train_df.columns = [\n",
    "    'id', 'brand', 'material', 'size', 'compartments', \n",
    "    'laptop_compartment', 'is_waterproof', 'style', 'color', \n",
    "    'weight_capacity', 'price'\n",
    "]\n",
    "test_df.columns = [\n",
    "    'id', 'brand', 'material', 'size', 'compartments', \n",
    "    'laptop_compartment', 'is_waterproof', 'style', 'color', \n",
    "    'weight_capacity',\n",
    "]\n",
    "\n",
    "# Define the mapping for Size conversion\n",
    "size_mapping = {\"Small\": 1, \"Medium\": 2, \"Large\": 3}\n",
    "train_df[\"size_int\"] = train_df[\"size\"].map(size_mapping)\n",
    "train_df['size_int'] = train_df['size_int'].fillna(0).astype(int)\n",
    "\n",
    "# Weight capacity features\n",
    "train_df['weight_capacity'] = train_df['weight_capacity'].fillna(0)\n",
    "train_df['weight_capacity_int'] = train_df['weight_capacity'].astype(int)\n",
    "train_df['weight_capacity_size'] = train_df['weight_capacity'] * train_df['size_int']\n",
    "\n",
    "train_df['compartments'] = train_df['compartments'].astype('category')\n",
    "num_cols = ['weight_capacity']\n",
    "cat_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "train_df[cat_cols] = train_df[cat_cols].astype('category')\n",
    "test_df[cat_cols] = test_df[cat_cols].astype('category')\n",
    "\n",
    "# Convert boolean columns to integer type\n",
    "train_df['laptop_compartment'] = train_df['laptop_compartment'].cat.codes.fillna(-1).astype(int)\n",
    "train_df['is_waterproof'] = train_df['is_waterproof'].cat.codes.astype(int)\n",
    "\n",
    "target = 'price'\n",
    "model_features = [\n",
    "    'weight_capacity', 'color', 'compartments', 'brand', 'material', 'is_waterproof'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "train_df['weight_capacity_pca'] = pca.fit_transform(train_df[['weight_capacity']])\n",
    "# train_df['weight_capacity_pca2'] = pca.fit_transform(train_df[['weight_capacity', 'color', 'compartments', 'brand']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=[target])\n",
    "y = train_df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['weight_capacity_int'] = train_df['weight_capacity'].astype(int)\n",
    "train_df['weight_capacity_size'] = train_df['weight_capacity'] * train_df['size_int']\n",
    "\n",
    "train_df['weight_capacity_binned'] = pd.qcut(train_df['weight_capacity'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# Interaction Terms\n",
    "train_df['weight_capacity_brand'] = train_df['weight_capacity'] * train_df['brand'].astype('category').cat.codes\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "train_df[['weight_capacity_poly_2']] = poly.fit_transform(train_df[['weight_capacity']])[:, 1:2]  # squared term\n",
    "train_df[['weight_capacity_poly_3']] = poly.transform(train_df[['weight_capacity']])[:, 2:]\n",
    "\n",
    "# Exponential\n",
    "train_df['weight_capacity_exp'] = np.exp(train_df['weight_capacity'])\n",
    "\n",
    "# Reciprocal Transformations\n",
    "train_df['weight_capacity_inv'] = 1 / (train_df['weight_capacity'] + 1e-6)  # Avoid division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_features(models, X, y, kf, verbose=True):\n",
    "    model_scores = {name: [] for name in models.keys()}\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'verbose': -1,\n",
    "        'force_row_wise': True\n",
    "    }\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "        if verbose:\n",
    "            print(f\"Starting Fold {fold}...\")\n",
    "        fold_start_time = time.time()\n",
    "\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        for name, model_features in models.items():\n",
    "            model_start_time = time.time()\n",
    "            \n",
    "            train_data = lgb.Dataset(X_train[model_features], label=y_train)\n",
    "            valid_data = lgb.Dataset(X_valid[model_features], label=y_valid, reference=train_data)\n",
    "            fit_model = lgb.train(params, train_data, num_boost_round=100, valid_sets=[valid_data])\n",
    "            y_pred = fit_model.predict(X_valid[model_features], num_iteration=fit_model.best_iteration)\n",
    "            \n",
    "            rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
    "            model_scores[name].append(rmse)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"{name} Model - Fold {fold} - Training & Prediction time: {time.time() - model_start_time:.2f} seconds\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Total time for Fold {fold}: {time.time() - fold_start_time:.2f} seconds\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    return pd.DataFrame(model_scores)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_features = ['weight_capacity', 'color', 'compartments', 'brand', 'material', 'is_waterproof']\n",
    "features_to_try = [\n",
    "    'weight_capacity_int', 'weight_capacity_size', \n",
    "    'weight_capacity_binned', 'weight_capacity_brand', 'weight_capacity_poly_2', \n",
    "    'weight_capacity_poly_3','weight_capacity_exp', 'weight_capacity_inv', \n",
    "    'weight_capacity_pca'] # , 'weight_capacity_density'\n",
    "\n",
    "models = {\n",
    "    \"baseline\": baseline_features,\n",
    "    \"weight_capacity_int\": ['weight_capacity_int'] + baseline_features,\n",
    "    \"weight_capacity_size\": ['weight_capacity_size'] + baseline_features,\n",
    "    \"weight_capacity_binned\": ['weight_capacity_binned'] + baseline_features,\n",
    "    \"weight_capacity_brand\": ['weight_capacity_brand'] + baseline_features,\n",
    "    \"weight_capacity_poly_2\": ['weight_capacity_poly_2'] + baseline_features,\n",
    "    \"weight_capacity_poly_3\": ['weight_capacity_poly_3'] + baseline_features,\n",
    "    \"weight_capacity_exp\": ['weight_capacity_exp'] + baseline_features,\n",
    "    \"weight_capacity_inv\": ['weight_capacity_inv'] + baseline_features,\n",
    "    # \"weight_capacity_density\": ['weight_capacity_density'] + baseline_features,\n",
    "    \"weight_capacity_pca\": ['weight_capacity_pca'] + baseline_features,\n",
    "    \n",
    "    # Combination models\n",
    "    \"poly_features\": ['weight_capacity_poly_2', 'weight_capacity_poly_3'] + baseline_features,\n",
    "    \"transformed_features\": ['weight_capacity_exp', 'weight_capacity_inv'] + baseline_features,\n",
    "    \"interaction_features\": ['weight_capacity_brand', 'weight_capacity_size'] + baseline_features,\n",
    "    # \"density_pca_features\": ['weight_capacity_density', 'weight_capacity_pca', 'weight_capacity'] + baseline_features,\n",
    "    \"all_features\": features_to_try + baseline_features  # Full model\n",
    "}\n",
    "\n",
    "result_df = cross_validate_features(models, X, y, kf, verbose=True)\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Mean RMSE\": result_df.mean(),\n",
    "    \"Std RMSE\": result_df.std()\n",
    "})\n",
    "display(summary_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
