{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, chisquare, kruskal, ks_2samp, chi2_contingency\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.cluster import MiniBatchKMeans, AffinityPropagation\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, FunctionTransformer\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "test_df = pd.read_csv(r'..//data//test.csv')\n",
    "train_df = pd.read_csv(r'..//data//train.csv')\n",
    "# train_extra_df = pd.read_csv(r'..//data//training_extra.csv')\n",
    "# train_df = pd.concat([train_df, train_extra_df], ignore_index=True)\n",
    "\n",
    "\n",
    "target = 'price'\n",
    "\n",
    "def prepare_data(df: pd.DataFrame, is_train: bool = True):\n",
    "    \"\"\"\n",
    "    Prepares the dataset for training or testing by renaming columns, handling missing values,\n",
    "    converting categorical and numerical features, and creating new features.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe (train or test).\n",
    "        is_train (bool): Indicates if the dataframe is training data (default is True).\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The processed dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the column names\n",
    "    columns = [\n",
    "        'id', 'brand', 'material', 'size', 'compartments', \n",
    "        'laptop_compartment', 'is_waterproof', 'style', 'color', \n",
    "        'weight_capacity'\n",
    "    ]\n",
    "    \n",
    "    if is_train:\n",
    "        columns.append('price')\n",
    "    \n",
    "    df.columns = columns\n",
    "    df = df.drop(columns='id')\n",
    "    \n",
    "    # Define the mapping for Size conversion\n",
    "    size_mapping = {\"Small\": 1, \"Medium\": 2, \"Large\": 3}\n",
    "    df[\"size_int\"] = df[\"size\"].map(size_mapping).fillna(0).astype(int)\n",
    "    \n",
    "    # Handle weight capacity\n",
    "    df['weight_capacity'] = df['weight_capacity'].fillna(0)\n",
    "    df['weight_capacity_int'] = df['weight_capacity'].astype(int)\n",
    "    df['weight_capacity_size'] = df['weight_capacity'] * df['size_int']\n",
    "    \n",
    "    # Convert categorical columns\n",
    "    df['compartments'] = df['compartments'].astype('category')\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    df[cat_cols] = df[cat_cols].astype('category')\n",
    "    \n",
    "    # Convert boolean columns to integer type\n",
    "    df['laptop_compartment'] = df['laptop_compartment'].cat.codes.fillna(-1).astype(int)\n",
    "    df['is_waterproof'] = df['is_waterproof'].cat.codes.fillna(-1).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply function to train and test datasets\n",
    "train_df = prepare_data(train_df, is_train=True)\n",
    "test_df = prepare_data(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = [\n",
    "    'weight_capacity', 'weight_capacity_int', 'weight_capacity_size', 'size_int', 'color', 'compartments', 'brand', 'material', 'is_waterproof'\n",
    "]\n",
    "numeric_cols = ['weight_capacity', 'weight_capacity_int', 'weight_capacity_size', 'size_int', 'is_waterproof']\n",
    "cat_cols = ['brand', 'material', 'compartments', 'color']\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[('scaler', StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[('onehot', OneHotEncoder(handle_unknown='ignore', drop=None))]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder='drop'  # drop other columns not listed\n",
    ")\n",
    "\n",
    "X = train_df[model_features].copy()\n",
    "y = train_df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniBatchKMeans Training & Prediction time: 0.83 seconds\n",
      "MiniBatchKMeans cluster counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>70538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>68946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>55846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>54823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>49847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  count\n",
       "0      4  70538\n",
       "1      2  68946\n",
       "2      1  55846\n",
       "3      3  54823\n",
       "4      0  49847"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### MiniBatchKMeans\n",
    "model_start_time = time.time()\n",
    "kmeans_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('clusterer', MiniBatchKMeans(n_clusters=5, random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "kmeans_pipeline.fit(X)\n",
    "kmeans_labels = kmeans_pipeline.named_steps['clusterer'].labels_\n",
    "train_df['cluster'] = kmeans_labels\n",
    "print(f\"MiniBatchKMeans Training & Prediction time: {time.time() - model_start_time:.2f} seconds\")\n",
    "\n",
    "print(\"MiniBatchKMeans cluster counts:\")\n",
    "display(pd.Series(kmeans_labels).value_counts().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_clusters(cluster_labels_dict, X, y, kf, verbose=True):\n",
    "    # LightGBM parameters\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'verbose': -1,\n",
    "        'force_row_wise': True\n",
    "    }\n",
    "\n",
    "    # This will store, for each clustering method, the RMSEs per model across folds\n",
    "    all_results = {}\n",
    "    # X = train_df[['cluster'] + model_features].copy()\n",
    "    \n",
    "    # Prepare a dict to accumulate fold RMSE scores for each model\n",
    "    model_scores = {name: [] for name in cluster_labels_dict.keys()}\n",
    "\n",
    "    # K-fold cross validation\n",
    "    for fold, (train_index, valid_index) in enumerate(kf.split(train_df), start=1):\n",
    "\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "        \n",
    "        # Loop over each clustering method and its cluster labels\n",
    "        for method_name, n_clusters in cluster_labels_dict.items():\n",
    "\n",
    "            if n_clusters == 1:\n",
    "                X_train['cluster'] = 0\n",
    "                X_valid['cluster'] = 0\n",
    "            else:\n",
    "                kmeans_pipeline = Pipeline(\n",
    "                    steps=[\n",
    "                        ('preprocessing', preprocessor),\n",
    "                        ('clusterer', MiniBatchKMeans(n_clusters=n_clusters, random_state=42))\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                kmeans_pipeline.fit(X_train)\n",
    "                kmeans_labels = kmeans_pipeline.named_steps['clusterer'].labels_\n",
    "                X_train['cluster'] = kmeans_labels\n",
    "                X_valid['cluster'] = kmeans_pipeline.predict(X_valid)\n",
    "\n",
    "            y_pred_valid = y_valid.reset_index()\n",
    "            y_pred_valid.loc[:, 'y_pred'] = 0\n",
    "            y_pred_valid = y_pred_valid.set_index('index')\n",
    "    \n",
    "            # Train a separate model for each cluster\n",
    "            for c in range(n_clusters):\n",
    "                train_cluster = X_train[X_train.cluster == c].copy()\n",
    "                val_cluster = X_valid[X_valid.cluster == c].copy()\n",
    "                \n",
    "                train_data = lgb.Dataset(train_cluster[model_features], \n",
    "                                         label=y_train[X_train.cluster == c])\n",
    "    \n",
    "                fit_model = lgb.train(\n",
    "                    params,\n",
    "                    train_data,\n",
    "                    num_boost_round=100,\n",
    "                    valid_sets=[train_data],\n",
    "                )\n",
    "                \n",
    "                y_pred_valid.loc[X_valid.cluster == c, 'y_pred'] = fit_model.predict(val_cluster[model_features], num_iteration=fit_model.best_iteration)\n",
    "    \n",
    "            # Calculate RMSE for this model across all validation samples\n",
    "            rmse = np.sqrt(mean_squared_error(y_valid, y_pred_valid['y_pred']))\n",
    "            model_scores[method_name].append(rmse)\n",
    "\n",
    "        # Summarize scores for this method\n",
    "        df_scores = pd.DataFrame(model_scores)\n",
    "        all_results[method_name] = df_scores\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans per-fold RMSE:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>kmeans_2</th>\n",
       "      <th>kmeans_3</th>\n",
       "      <th>kmeans_4</th>\n",
       "      <th>kmeans_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38.913</td>\n",
       "      <td>38.927</td>\n",
       "      <td>38.933</td>\n",
       "      <td>38.954</td>\n",
       "      <td>38.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.040</td>\n",
       "      <td>39.057</td>\n",
       "      <td>39.064</td>\n",
       "      <td>39.064</td>\n",
       "      <td>39.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.016</td>\n",
       "      <td>39.022</td>\n",
       "      <td>39.038</td>\n",
       "      <td>39.052</td>\n",
       "      <td>39.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.062</td>\n",
       "      <td>39.070</td>\n",
       "      <td>39.082</td>\n",
       "      <td>39.086</td>\n",
       "      <td>39.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.003</td>\n",
       "      <td>39.010</td>\n",
       "      <td>39.024</td>\n",
       "      <td>39.029</td>\n",
       "      <td>39.046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   baseline  kmeans_2  kmeans_3  kmeans_4  kmeans_5\n",
       "0    38.913    38.927    38.933    38.954    38.968\n",
       "1    39.040    39.057    39.064    39.064    39.078\n",
       "2    39.016    39.022    39.038    39.052    39.080\n",
       "3    39.062    39.070    39.082    39.086    39.097\n",
       "4    39.003    39.010    39.024    39.029    39.046"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean RMSE</th>\n",
       "      <th>Std RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>39.007</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kmeans_2</th>\n",
       "      <td>39.017</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kmeans_3</th>\n",
       "      <td>39.028</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kmeans_4</th>\n",
       "      <td>39.037</td>\n",
       "      <td>0.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kmeans_5</th>\n",
       "      <td>39.054</td>\n",
       "      <td>0.051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Mean RMSE  Std RMSE\n",
       "baseline     39.007     0.057\n",
       "kmeans_2     39.017     0.056\n",
       "kmeans_3     39.028     0.058\n",
       "kmeans_4     39.037     0.051\n",
       "kmeans_5     39.054     0.051"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels_dict = {\n",
    "    \"baseline\": 1,\n",
    "    \"kmeans_2\": 2,\n",
    "    \"kmeans_3\": 3,\n",
    "    \"kmeans_4\": 4,\n",
    "    \"kmeans_5\": 5,\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cluster_cv_results = cv_clusters(cluster_labels_dict, X, y, kf, verbose=True)\n",
    "\n",
    "# Inspect results\n",
    "result_kmeans = cluster_cv_results[\"kmeans_5\"]\n",
    "print(\"KMeans per-fold RMSE:\")\n",
    "# display(result_kmeans.round(3))\n",
    "\n",
    "# Summarize\n",
    "summary_kmeans = pd.DataFrame({\n",
    "    \"Mean RMSE\": result_kmeans.mean(),\n",
    "    \"Std RMSE\": result_kmeans.std()\n",
    "})\n",
    "summary_kmeans.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, chisquare, kruskal, ks_2samp, chi2_contingency\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, FunctionTransformer\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "train_df['weight_capacity_pca'] = pca.fit_transform(train_df[['weight_capacity']])\n",
    "# train_df['weight_capacity_pca2'] = pca.fit_transform(train_df[['weight_capacity', 'color', 'compartments', 'brand']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=[target])\n",
    "y = train_df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['weight_capacity_int'] = train_df['weight_capacity'].astype(int)\n",
    "train_df['weight_capacity_size'] = train_df['weight_capacity'] * train_df['size_int']\n",
    "\n",
    "train_df['weight_capacity_binned'] = pd.qcut(train_df['weight_capacity'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# Interaction Terms\n",
    "train_df['weight_capacity_brand'] = train_df['weight_capacity'] * train_df['brand'].astype('category').cat.codes\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "train_df[['weight_capacity_poly_2']] = poly.fit_transform(train_df[['weight_capacity']])[:, 1:2]  # squared term\n",
    "train_df[['weight_capacity_poly_3']] = poly.transform(train_df[['weight_capacity']])[:, 2:]\n",
    "\n",
    "# Exponential\n",
    "train_df['weight_capacity_exp'] = np.exp(train_df['weight_capacity'])\n",
    "\n",
    "# Reciprocal Transformations\n",
    "train_df['weight_capacity_inv'] = 1 / (train_df['weight_capacity'] + 1e-6)  # Avoid division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_features(models, X, y, kf, verbose=True):\n",
    "    model_scores = {name: [] for name in models.keys()}\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'verbose': -1,\n",
    "        'force_row_wise': True\n",
    "    }\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "        if verbose:\n",
    "            print(f\"Starting Fold {fold}...\")\n",
    "        fold_start_time = time.time()\n",
    "\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        for name, model_features in models.items():\n",
    "            model_start_time = time.time()\n",
    "            \n",
    "            train_data = lgb.Dataset(X_train[model_features], label=y_train)\n",
    "            valid_data = lgb.Dataset(X_valid[model_features], label=y_valid, reference=train_data)\n",
    "            fit_model = lgb.train(params, train_data, num_boost_round=100, valid_sets=[valid_data])\n",
    "            y_pred = fit_model.predict(X_valid[model_features], num_iteration=fit_model.best_iteration)\n",
    "            \n",
    "            rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
    "            model_scores[name].append(rmse)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"{name} Model - Fold {fold} - Training & Prediction time: {time.time() - model_start_time:.2f} seconds\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Total time for Fold {fold}: {time.time() - fold_start_time:.2f} seconds\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    return pd.DataFrame(model_scores)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 1...\n",
      "baseline Model - Fold 1 - Training & Prediction time: 30.53 seconds\n",
      "weight_capacity_int Model - Fold 1 - Training & Prediction time: 29.68 seconds\n",
      "weight_capacity_size Model - Fold 1 - Training & Prediction time: 29.20 seconds\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['weight_capacity_binned'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m\n\u001b[1;32m      2\u001b[0m features_to_try \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_capacity_int\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_capacity_size\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_capacity_binned\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_capacity_brand\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_capacity_poly_2\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_capacity_poly_3\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_capacity_exp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_capacity_inv\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_capacity_pca\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# , 'weight_capacity_density'\u001b[39;00m\n\u001b[1;32m      8\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline\u001b[39m\u001b[38;5;124m\"\u001b[39m: baseline_features,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_capacity_int\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_capacity_int\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m baseline_features,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: features_to_try \u001b[38;5;241m+\u001b[39m baseline_features  \u001b[38;5;66;03m# Full model\u001b[39;00m\n\u001b[1;32m     27\u001b[0m }\n\u001b[0;32m---> 29\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m summary_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean RMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m: result_df\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStd RMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m: result_df\u001b[38;5;241m.\u001b[39mstd()\n\u001b[1;32m     33\u001b[0m })\n\u001b[1;32m     34\u001b[0m display(summary_df)\n",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m, in \u001b[0;36mcross_validate_features\u001b[0;34m(models, X, y, kf, verbose)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model_features \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     23\u001b[0m     model_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 25\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_features\u001b[49m\u001b[43m]\u001b[49m, label\u001b[38;5;241m=\u001b[39my_train)\n\u001b[1;32m     26\u001b[0m     valid_data \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(X_valid[model_features], label\u001b[38;5;241m=\u001b[39my_valid, reference\u001b[38;5;241m=\u001b[39mtrain_data)\n\u001b[1;32m     27\u001b[0m     fit_model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mtrain(params, train_data, num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, valid_sets\u001b[38;5;241m=\u001b[39m[valid_data])\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['weight_capacity_binned'] not in index\""
     ]
    }
   ],
   "source": [
    "baseline_features = ['weight_capacity', 'color', 'compartments', 'brand', 'material', 'is_waterproof']\n",
    "features_to_try = [\n",
    "    'weight_capacity_int', 'weight_capacity_size', \n",
    "    'weight_capacity_binned', 'weight_capacity_brand', 'weight_capacity_poly_2', \n",
    "    'weight_capacity_poly_3','weight_capacity_exp', 'weight_capacity_inv', \n",
    "    'weight_capacity_pca'] # , 'weight_capacity_density'\n",
    "\n",
    "models = {\n",
    "    \"baseline\": baseline_features,\n",
    "    \"weight_capacity_int\": ['weight_capacity_int'] + baseline_features,\n",
    "    \"weight_capacity_size\": ['weight_capacity_size'] + baseline_features,\n",
    "    \"weight_capacity_binned\": ['weight_capacity_binned'] + baseline_features,\n",
    "    \"weight_capacity_brand\": ['weight_capacity_brand'] + baseline_features,\n",
    "    \"weight_capacity_poly_2\": ['weight_capacity_poly_2'] + baseline_features,\n",
    "    \"weight_capacity_poly_3\": ['weight_capacity_poly_3'] + baseline_features,\n",
    "    \"weight_capacity_exp\": ['weight_capacity_exp'] + baseline_features,\n",
    "    \"weight_capacity_inv\": ['weight_capacity_inv'] + baseline_features,\n",
    "    # \"weight_capacity_density\": ['weight_capacity_density'] + baseline_features,\n",
    "    \"weight_capacity_pca\": ['weight_capacity_pca'] + baseline_features,\n",
    "    \n",
    "    # Combination models\n",
    "    \"poly_features\": ['weight_capacity_poly_2', 'weight_capacity_poly_3'] + baseline_features,\n",
    "    \"transformed_features\": ['weight_capacity_exp', 'weight_capacity_inv'] + baseline_features,\n",
    "    \"interaction_features\": ['weight_capacity_brand', 'weight_capacity_size'] + baseline_features,\n",
    "    # \"density_pca_features\": ['weight_capacity_density', 'weight_capacity_pca', 'weight_capacity'] + baseline_features,\n",
    "    \"all_features\": features_to_try + baseline_features  # Full model\n",
    "}\n",
    "\n",
    "result_df = cross_validate_features(models, X, y, kf, verbose=True)\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Mean RMSE\": result_df.mean(),\n",
    "    \"Std RMSE\": result_df.std()\n",
    "})\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
