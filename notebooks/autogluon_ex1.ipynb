{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from backpack_predictor import prepare_data, target_encoding\n",
    "from backpack_predictor.features import target, baseline_features, feature_list, cat_cols\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, chisquare, kruskal, ks_2samp, chi2_contingency\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "test_df = pd.read_csv(r'..//data//test.csv')\n",
    "train_df = pd.read_csv(r'..//data//train.csv')\n",
    "train_extra_df = pd.read_csv(r'..//data//training_extra.csv')\n",
    "train_df = pd.concat([train_df, train_extra_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Apply function to train and test datasets\n",
    "train_df = prepare_data(train_df, is_train=True)\n",
    "test_df = prepare_data(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['brand', 'material', 'size', 'compartments', 'laptop_compartment',\n",
       "       'is_waterproof', 'style', 'color', 'weight_capacity', 'price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [\n",
    "    'compartments', 'weight_capacity_te', 'laptop_compartment', 'is_waterproof',\n",
    "]+ [target]\n",
    "\n",
    "feature_list = ['brand', 'material', 'size', 'compartments', 'laptop_compartment',\n",
    "       'is_waterproof', 'style', 'color', 'weight_capacity', 'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250214_033422\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preset alias specified: 'best' maps to 'best_quality'.\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.10\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\n",
      "CPU Count:          10\n",
      "Memory Avail:       28.61 GB / 64.00 GB (44.7%)\n",
      "Disk Space Avail:   457.75 GB / 926.35 GB (49.4%)\n",
      "===================================================\n",
      "Presets specified: ['best']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 6300s of the 25200s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-02-13 21:34:24,573\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250214_033422/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Beginning AutoGluon training ... Time limit = 6298s\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m AutoGluon will save models to \"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250214_033422/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Train Data Rows:    3550504\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Train Data Columns: 9\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Label Column:       price\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tAvailable Memory:                    29597.36 MB\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tTrain Data (Original)  Memory Usage: 243.79 MB (0.8% of available memory)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t\t('float', []) : 1 | ['weight_capacity']\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t\t('int', [])   : 8 | ['brand', 'material', 'size', 'compartments', 'laptop_compartment', ...]\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t\t('float', []) : 1 | ['weight_capacity']\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t\t('int', [])   : 8 | ['brand', 'material', 'size', 'compartments', 'laptop_compartment', ...]\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t2.3s = Fit runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t9 features in original data used to generate 9 features in processed data.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tTrain Data (Processed) Memory Usage: 243.79 MB (0.8% of available memory)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Data preprocessing and feature engineering runtime = 2.48s ...\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 4195.87s of the 6295.38s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-42.583\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t5.09s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t135.83s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 4053.63s of the 6153.13s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-45.2758\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t5.3s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t141.66s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3906.12s of the 6005.63s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.88%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=46970)\u001b[0m [1000]\tvalid_set's rmse: 38.8629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.8822\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t148.29s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t154.16s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 3732.70s of the 5832.21s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.16%)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.8704\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t71.91s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t70.57s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 3647.32s of the 5746.82s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.8772\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t592.63s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t63.82s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 2990.22s of the 5089.73s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.38%)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.8706\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t583.58s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t0.87s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 2404.30s of the 4503.81s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.8917\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t226.05s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t57.71s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2119.89s of the 4219.39s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.25%)\n",
      "\u001b[36m(_ray_fit pid=47665)\u001b[0m No improvement since epoch 1: early stopping\n",
      "\u001b[36m(_ray_fit pid=47659)\u001b[0m No improvement since epoch 1: early stopping\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=47660)\u001b[0m No improvement since epoch 2: early stopping\n",
      "\u001b[36m(_ray_fit pid=47663)\u001b[0m No improvement since epoch 2: early stopping\n",
      "\u001b[36m(_ray_fit pid=47662)\u001b[0m No improvement since epoch 3: early stopping\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.9131\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t1026.59s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t8.44s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 1089.64s of the 3189.15s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.61%)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.8713\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t72.39s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t26.54s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1010.71s of the 3110.21s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.53%)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.9118\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t725.67s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t2.58s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 282.03s of the 2381.54s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.80%)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.8701\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t72.94s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t79.36s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 194.96s of the 2294.46s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.81%)\n",
      "\u001b[36m(_ray_fit pid=48435)\u001b[0m \tRan out of time, early stopping on iteration 451.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.8726\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t155.34s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t0.75s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 37.27s of the 2136.77s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.59%)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tTime limit exceeded... Skipping NeuralNetTorch_r79_BAG_L1.\n",
      "\u001b[36m(_ray_fit pid=48434)\u001b[0m \tRan out of time, early stopping on iteration 453.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=48496)\u001b[0m \tNot enough time to train first epoch. (Time Required: 26.07s, Time Left: 24.46s)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 29.20s of the 2128.71s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.81%)\n",
      "\u001b[36m(_ray_fit pid=48513)\u001b[0m \tRan out of time, early stopping on iteration 100. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=48513)\u001b[0m \t[100]\tvalid_set's rmse: 38.9018\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.9002\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t23.42s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t24.19s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 0.46s of the 2099.96s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.37%)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tTime limit exceeded... Skipping NeuralNetFastAI_r191_BAG_L1.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 419.59s of the 2098.74s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tEnsemble Weights: {'RandomForestMSE_BAG_L1': 0.36, 'XGBoost_BAG_L1': 0.24, 'LightGBM_BAG_L1': 0.2, 'CatBoost_BAG_L1': 0.2}\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.8628\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t0.9s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_ray_fit pid=48509)\u001b[0m \tRan out of time, early stopping on iteration 99. Best iteration is:\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=48509)\u001b[0m \t[99]\tvalid_set's rmse: 38.9364\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2097.68s of the 2097.42s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.06%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=48529)\u001b[0m [1000]\tvalid_set's rmse: 38.8586\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=48529)\u001b[0m [2000]\tvalid_set's rmse: 38.8589\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.8602\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t227.4s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t102.51s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 1844.60s of the 1844.34s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.79%)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.862\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t35.19s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t18.17s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 1802.56s of the 1802.30s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 53 due to low time. Expected time usage reduced from 10058.2s -> 1800.2s...\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.9249\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t985.1s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t17.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 798.97s of the 798.71s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.98%)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.8616\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t528.16s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t0.62s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 266.68s of the 266.42s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 90 due to low time. Expected time usage reduced from 871.1s -> 264.3s...\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.8688\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t168.44s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t18.27s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 78.57s of the 78.31s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.03% memory usage per fold, 56.12%/80.00% total).\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=14.03%)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L2.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 60.42s of the 60.16s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.91%)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.8621\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t46.66s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t7.35s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 7.54s of remaining time.\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.545, 'ExtraTreesMSE_BAG_L2': 0.273, 'RandomForestMSE_BAG_L2': 0.091, 'RandomForestMSE_BAG_L1': 0.045, 'LightGBM_BAG_L2': 0.045}\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t-38.8559\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t1.6s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m AutoGluon training complete, total runtime = 6292.2s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 818.2 rows/s (443813 batch size)\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250214_033422/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=46504)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L3     -38.849186 -38.855908  root_mean_squared_error      118.097492     922.586676  5126.925844                 0.005347                0.031034           1.601533            3       True         21\n",
      "1        LightGBMXT_BAG_L2     -38.850573 -38.860194  root_mean_squared_error      114.491555     868.990731  3936.594047                12.869372              102.505346         227.395740            2       True         15\n",
      "2          CatBoost_BAG_L2     -38.851368 -38.861581  root_mean_squared_error      102.188725     767.106664  4237.361561                 0.566542                0.621279         528.163254            2       True         18\n",
      "3          LightGBM_BAG_L2     -38.851406 -38.861955  root_mean_squared_error      104.158545     784.654832  3744.387906                 2.536362               18.169446          35.189599            2       True         16\n",
      "4           XGBoost_BAG_L2     -38.851481 -38.862149  root_mean_squared_error      104.362444     773.838933  3755.859923                 2.740261                7.353548          46.661616            2       True         20\n",
      "5      WeightedEnsemble_L2     -38.852630 -38.862842  root_mean_squared_error       16.062353     161.825282  1321.412564                 0.006288                0.026530           0.903720            2       True         14\n",
      "6     ExtraTreesMSE_BAG_L2     -38.854890 -38.868830  root_mean_squared_error      102.205589     784.759272  3877.641964                 0.583406               18.273887         168.443657            2       True         19\n",
      "7          LightGBM_BAG_L1     -38.858047 -38.870389  root_mean_squared_error        8.827334      70.570905    71.914741                 8.827334               70.570905          71.914741            1       True          4\n",
      "8     LightGBMLarge_BAG_L1     -38.858391 -38.870097  root_mean_squared_error        9.414976      79.358339    72.940031                 9.414976               79.358339          72.940031            1       True         11\n",
      "9          CatBoost_BAG_L1     -38.859022 -38.870587  root_mean_squared_error        1.055627       0.869569   583.577339                 1.055627                0.869569         583.577339            1       True          6\n",
      "10          XGBoost_BAG_L1     -38.859049 -38.871283  root_mean_squared_error        4.061185      26.542025    72.390691                 4.061185               26.542025          72.390691            1       True          9\n",
      "11    CatBoost_r177_BAG_L1     -38.861905 -38.872583  root_mean_squared_error        0.382580       0.746520   155.340359                 0.382580                0.746520         155.340359            1       True         12\n",
      "12  RandomForestMSE_BAG_L1     -38.864361 -38.877212  root_mean_squared_error        2.111919      63.816254   592.626073                 2.111919               63.816254         592.626073            1       True          5\n",
      "13  RandomForestMSE_BAG_L2     -38.872103 -38.924853  root_mean_squared_error      102.103005     783.606962  4694.295315                 0.480822               17.121577         985.097008            2       True         17\n",
      "14       LightGBMXT_BAG_L1     -38.872250 -38.882184  root_mean_squared_error       18.061160     154.162709   148.285592                18.061160              154.162709         148.285592            1       True          3\n",
      "15    ExtraTreesMSE_BAG_L1     -38.883435 -38.891693  root_mean_squared_error        1.757151      57.712112   226.052543                 1.757151               57.712112         226.052543            1       True          7\n",
      "16    LightGBM_r131_BAG_L1     -38.891246 -38.900227  root_mean_squared_error        3.116042      24.194857    23.416768                 3.116042               24.194857          23.416768            1       True         13\n",
      "17   NeuralNetTorch_BAG_L1     -38.898882 -38.911819  root_mean_squared_error        2.829768       2.577820   725.673749                 2.829768                2.577820         725.673749            1       True         10\n",
      "18  NeuralNetFastAI_BAG_L1     -38.899496 -38.913078  root_mean_squared_error       14.075507       8.437345  1026.587120                14.075507                8.437345        1026.587120            1       True          8\n",
      "19   KNeighborsUnif_BAG_L1     -42.617434 -42.582983  root_mean_squared_error       18.066965     135.833537     5.089937                18.066965              135.833537           5.089937            1       True          1\n",
      "20   KNeighborsDist_BAG_L1     -45.325088 -45.275790  root_mean_squared_error       17.861969     141.663394     5.303365                17.861969              141.663394           5.303365            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t6418s\t = DyStack   runtime |\t18782s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 18782s\n",
      "AutoGluon will save models to \"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250214_033422\"\n",
      "Train Data Rows:    3994318\n",
      "Train Data Columns: 9\n",
      "Label Column:       price\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    36868.00 MB\n",
      "\tTrain Data (Original)  Memory Usage: 274.27 MB (0.7% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 1 | ['weight_capacity']\n",
      "\t\t('int', [])   : 8 | ['brand', 'material', 'size', 'compartments', 'laptop_compartment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 1 | ['weight_capacity']\n",
      "\t\t('int', [])   : 8 | ['brand', 'material', 'size', 'compartments', 'laptop_compartment', ...]\n",
      "\t2.5s = Fit runtime\n",
      "\t9 features in original data used to generate 9 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 274.27 MB (0.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.83s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 12516.24s of the 18779.05s of remaining time.\n",
      "\t-42.5841\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.27s\t = Training   runtime\n",
      "\t154.36s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 12352.40s of the 18615.21s of remaining time.\n",
      "\t-45.3198\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.94s\t = Training   runtime\n",
      "\t154.42s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 12189.46s of the 18452.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.60%)\n",
      "\t-38.8802\t = Validation score   (-root_mean_squared_error)\n",
      "\t193.49s\t = Training   runtime\n",
      "\t200.9s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 11962.96s of the 18225.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.77%)\n",
      "\t-38.8675\t = Validation score   (-root_mean_squared_error)\n",
      "\t92.94s\t = Training   runtime\n",
      "\t95.14s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 11854.63s of the 18117.44s of remaining time.\n",
      "\t-38.873\t = Validation score   (-root_mean_squared_error)\n",
      "\t675.59s\t = Training   runtime\n",
      "\t68.2s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 11110.05s of the 17372.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.00%)\n",
      "\t-38.868\t = Validation score   (-root_mean_squared_error)\n",
      "\t894.55s\t = Training   runtime\n",
      "\t1.1s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 10213.25s of the 16476.06s of remaining time.\n",
      "\t-38.8889\t = Validation score   (-root_mean_squared_error)\n",
      "\t260.27s\t = Training   runtime\n",
      "\t62.65s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 9889.62s of the 16152.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.75%)\n",
      "\t-38.9141\t = Validation score   (-root_mean_squared_error)\n",
      "\t1160.64s\t = Training   runtime\n",
      "\t10.32s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 8725.39s of the 14988.20s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.15%)\n",
      "\t-38.8688\t = Validation score   (-root_mean_squared_error)\n",
      "\t80.19s\t = Training   runtime\n",
      "\t27.92s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 8639.03s of the 14901.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.42%)\n",
      "\t-38.9092\t = Validation score   (-root_mean_squared_error)\n",
      "\t1158.13s\t = Training   runtime\n",
      "\t2.92s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 7478.39s of the 13741.20s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.67%)\n",
      "\t-38.8683\t = Validation score   (-root_mean_squared_error)\n",
      "\t84.91s\t = Training   runtime\n",
      "\t93.67s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 7378.05s of the 13640.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.68%)\n",
      "\t-38.8683\t = Validation score   (-root_mean_squared_error)\n",
      "\t573.11s\t = Training   runtime\n",
      "\t0.84s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 6802.83s of the 13065.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.50%)\n",
      "\t-38.9117\t = Validation score   (-root_mean_squared_error)\n",
      "\t1432.59s\t = Training   runtime\n",
      "\t7.97s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 5367.39s of the 11630.21s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.70%)\n",
      "\t-38.8673\t = Validation score   (-root_mean_squared_error)\n",
      "\t381.08s\t = Training   runtime\n",
      "\t2300.36s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 4687.26s of the 10950.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.19%)\n",
      "\t-38.9222\t = Validation score   (-root_mean_squared_error)\n",
      "\t3207.61s\t = Training   runtime\n",
      "\t29.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 1474.21s of the 7737.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.57%)\n",
      "\t-38.87\t = Validation score   (-root_mean_squared_error)\n",
      "\t834.39s\t = Training   runtime\n",
      "\t5.33s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 636.90s of the 6899.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.73%)\n",
      "\t-38.8829\t = Validation score   (-root_mean_squared_error)\n",
      "\t510.24s\t = Training   runtime\n",
      "\t3005.89s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 1251.62s of the 6008.86s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 0.429, 'CatBoost_r177_BAG_L1': 0.286, 'LightGBM_r131_BAG_L1': 0.143, 'LightGBM_BAG_L1': 0.095, 'XGBoost_BAG_L1': 0.048}\n",
      "\t-38.8604\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.6s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 6007.05s of the 6006.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.22%)\n",
      "\t-38.8584\t = Validation score   (-root_mean_squared_error)\n",
      "\t153.43s\t = Training   runtime\n",
      "\t78.89s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 5835.75s of the 5835.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.91%)\n",
      "\t-38.8587\t = Validation score   (-root_mean_squared_error)\n",
      "\t61.08s\t = Training   runtime\n",
      "\t25.31s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 5765.30s of the 5764.94s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 116 due to low time. Expected time usage reduced from 14848.5s -> 5762.0s...\n",
      "\t-38.8825\t = Validation score   (-root_mean_squared_error)\n",
      "\t3125.12s\t = Training   runtime\n",
      "\t34.32s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 2603.95s of the 2603.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.10%)\n",
      "\t-38.8585\t = Validation score   (-root_mean_squared_error)\n",
      "\t795.18s\t = Training   runtime\n",
      "\t1.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 1804.12s of the 1803.75s of remaining time.\n",
      "\t-38.8539\t = Validation score   (-root_mean_squared_error)\n",
      "\t712.98s\t = Training   runtime\n",
      "\t71.85s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1017.32s of the 1016.96s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.21% memory usage per fold, 64.85%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=16.21%)\n",
      "\t-38.8607\t = Validation score   (-root_mean_squared_error)\n",
      "\t685.68s\t = Training   runtime\n",
      "\t13.54s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 325.89s of the 325.53s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.80% memory usage per fold, 47.20%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=11.80%)\n",
      "\t-38.8596\t = Validation score   (-root_mean_squared_error)\n",
      "\t60.88s\t = Training   runtime\n",
      "\t5.84s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 259.37s of the 259.01s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.58%)\n",
      "\t-38.8752\t = Validation score   (-root_mean_squared_error)\n",
      "\t204.16s\t = Training   runtime\n",
      "\t14.07s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 48.73s of the 48.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.58%)\n",
      "\t-38.8589\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.28s\t = Training   runtime\n",
      "\t30.47s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 600.70s of the 1.72s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE_BAG_L2': 0.6, 'RandomForestMSE_BAG_L1': 0.1, 'LightGBMXT_BAG_L2': 0.1, 'RandomForestMSE_BAG_L2': 0.1, 'CatBoost_BAG_L2': 0.05, 'NeuralNetTorch_BAG_L2': 0.05}\n",
      "\t-38.8522\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.45s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 18781.93s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 84.0 rows/s (499290 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250214_033422\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label=target, \n",
    "    problem_type='regression', \n",
    "    eval_metric='root_mean_squared_error'\n",
    ").fit(\n",
    "    train_df[feature_list], \n",
    "    presets='best',\n",
    "    time_limit=60 * 60 * 7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TabularPredictor.load(\n",
    "    \"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250214_033422\"\n",
    ")\n",
    "preds = predictor.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str = 'ag-20250214_03342'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to: ..//submissions//ag-20250214_03342.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300000</td>\n",
       "      <td>81.542183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300001</td>\n",
       "      <td>82.188515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300002</td>\n",
       "      <td>83.514381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300003</td>\n",
       "      <td>81.122070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300004</td>\n",
       "      <td>79.769196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      Price\n",
       "0  300000  81.542183\n",
       "1  300001  82.188515\n",
       "2  300002  83.514381\n",
       "3  300003  81.122070\n",
       "4  300004  79.769196"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_path = f'..//submissions//' + model_str + \".csv\"\n",
    "print(\"Saving to:\", submit_path)\n",
    "\n",
    "submit_df = test_df[['id']].copy()\n",
    "submit_df['Price'] = preds\n",
    "submit_df.to_csv(submit_path, index=False)\n",
    "submit_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
