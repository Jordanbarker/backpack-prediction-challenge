{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from backpack_predictor import prepare_data, target_encoding\n",
    "from backpack_predictor.features import target, baseline_features, feature_list, cat_cols\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, chisquare, kruskal, ks_2samp, chi2_contingency\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from optuna.integration import LightGBMPruningCallback #XGBoostPruningCallback, CatBoostPruningCallback\n",
    "import optuna\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "test_df = pd.read_csv(r'..//data//test.csv')\n",
    "train_df = pd.read_csv(r'..//data//train.csv')\n",
    "# train_extra_df = pd.read_csv(r'..//data//training_extra.csv')\n",
    "# train_df = pd.concat([train_df, train_extra_df], ignore_index=True)\n",
    "\n",
    "# Apply function to train and test datasets\n",
    "train_df = prepare_data(train_df, is_train=True)\n",
    "test_df = prepare_data(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>skew</th>\n",
       "      <th>var</th>\n",
       "      <th>count</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>17.774679</td>\n",
       "      <td>-0.088817</td>\n",
       "      <td>53.173022</td>\n",
       "      <td>9705</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.074264</td>\n",
       "      <td>-0.073533</td>\n",
       "      <td>48.141221</td>\n",
       "      <td>60077</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.949744</td>\n",
       "      <td>-0.049983</td>\n",
       "      <td>49.014051</td>\n",
       "      <td>56814</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.019246</td>\n",
       "      <td>-0.060796</td>\n",
       "      <td>47.981299</td>\n",
       "      <td>57336</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.000535</td>\n",
       "      <td>-0.068164</td>\n",
       "      <td>48.559100</td>\n",
       "      <td>56076</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.096995</td>\n",
       "      <td>-0.088020</td>\n",
       "      <td>48.947975</td>\n",
       "      <td>59992</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean      skew        var  count  min   max\n",
       "brand                                                  \n",
       "-1     17.774679 -0.088817  53.173022   9705 -1.0  30.0\n",
       " 0     18.074264 -0.073533  48.141221  60077 -1.0  30.0\n",
       " 1     17.949744 -0.049983  49.014051  56814 -1.0  30.0\n",
       " 2     18.019246 -0.060796  47.981299  57336 -1.0  30.0\n",
       " 3     18.000535 -0.068164  48.559100  56076 -1.0  30.0\n",
       " 4     18.096995 -0.088020  48.947975  59992 -1.0  30.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col1 = 'brand'\n",
    "# col1 = 'weight_capacity'\n",
    "# col2 = 'is_waterproof'\n",
    "col2 = 'weight_capacity'\n",
    "stats = ['mean', 'skew', 'var', 'count', 'min', 'max']\n",
    "\n",
    "agg_stats = train_df.groupby(col1)[col2].agg(stats)\n",
    "agg_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['brand', 'material', 'size', 'compartments', 'laptop_compartment',\n",
       "       'is_waterproof', 'style', 'color', 'weight_capacity', 'price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniBatchKMeans Training & Prediction time: 1.68 seconds\n",
      "MiniBatchKMeans cluster counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>214</td>\n",
       "      <td>1554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191</td>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>291</td>\n",
       "      <td>1193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59</td>\n",
       "      <td>1118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92</td>\n",
       "      <td>1103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>280</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>356</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>251</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>26</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>134</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  count\n",
       "0      214   1554\n",
       "1      191   1235\n",
       "2      291   1193\n",
       "3       59   1118\n",
       "4       92   1103\n",
       "..     ...    ...\n",
       "495    280    195\n",
       "496    356    188\n",
       "497    251    185\n",
       "498     26    179\n",
       "499    134    141\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "numeric_cols = ['weight_capacity']\n",
    "cat_cols = ['size', 'is_waterproof', 'brand', 'material', 'laptop_compartment', 'compartments', 'style', 'color']\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[('scaler', StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[('onehot', OneHotEncoder(handle_unknown='ignore', drop=None))]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder='drop'  # drop other columns not listed\n",
    ")\n",
    "\n",
    "### MiniBatchKMeans\n",
    "model_start_time = time.time()\n",
    "kmeans_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('clusterer', MiniBatchKMeans(n_clusters=500, random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "kmeans_pipeline.fit(train_df)\n",
    "kmeans_labels = kmeans_pipeline.named_steps['clusterer'].labels_\n",
    "train_df['cluster'] = kmeans_labels\n",
    "print(f\"MiniBatchKMeans Training & Prediction time: {time.time() - model_start_time:.2f} seconds\")\n",
    "\n",
    "print(\"MiniBatchKMeans cluster counts:\")\n",
    "display(pd.Series(kmeans_labels).value_counts().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add original data\n",
    "orig_df = pd.read_csv(r'..//data//orig.csv')\n",
    "orig_df.insert(loc=0, column='id', value=0) # Add id to first col to match new train\n",
    "orig_df = prepare_data(orig_df, is_train=True)\n",
    "orig_df.columns = [f\"{c}_orig\" for c in orig_df.columns]\n",
    "orig_df_columns = orig_df.columns.to_list()\n",
    "train_df = train_df.merge(orig_df.loc[(orig_df[\"weight_capacity_orig\"]>5)&(orig_df[\"weight_capacity_orig\"]<30)], left_on='weight_capacity', right_on='weight_capacity_orig', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str = \"lgb_\"\n",
    "study_name = 'many_cols_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def create_comb_features(train_df, test_df, cols, allowed_features, comb_size):\n",
    "    for comb in itertools.combinations(cols, comb_size):\n",
    "        col_name = \"_\".join(comb)\n",
    "        if col_name not in allowed_features:\n",
    "            continue\n",
    "        # Create the new feature by joining the selected columns row-wise\n",
    "        train_df[col_name] = train_df[list(comb)].astype(str).agg('_'.join, axis=1)\n",
    "        test_df[col_name] = test_df[list(comb)].astype(str).agg('_'.join, axis=1)\n",
    "\n",
    "\n",
    "best_2 = [\n",
    "    'material_is_waterproof', 'material_laptop_compartment',\n",
    "    'material_size', 'material_style', 'is_waterproof_color', 'style_color'\n",
    "]\n",
    "best_3 = [\n",
    "    'material_laptop_compartment_is_waterproof', 'material_laptop_compartment_style',\n",
    "    'material_is_waterproof_style', 'material_size_laptop_compartment', 'laptop_compartment_is_waterproof_color'\n",
    "]\n",
    "best_4 = [\n",
    "    'material_size_laptop_compartment_is_waterproof', 'material_laptop_compartment_is_waterproof_style',\n",
    "    'brand_laptop_compartment_is_waterproof_style', 'material_laptop_compartment_is_waterproof_color',\n",
    "    'brand_material_laptop_compartment_is_waterproof'\n",
    "]\n",
    "\n",
    "cols = ['brand', 'material', 'size', 'laptop_compartment', 'is_waterproof', 'style', 'color']\n",
    "create_comb_features(train_df, test_df, cols, best_2, 2)\n",
    "create_comb_features(train_df, test_df, cols, best_3, 3)\n",
    "create_comb_features(train_df, test_df, cols, best_4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ['mean', 'skew', 'count', 'var']\n",
    "FOLDS = 3\n",
    "inner_folds = 10\n",
    "\n",
    "target_feature_cols = ['weight_capacity', 'compartments', 'laptop_compartment', 'is_waterproof', 'material', 'brand']\n",
    "target_feature_cols = target_feature_cols + best_2 + best_3 + best_4 + orig_df_columns \n",
    "target_feature_sets = {\n",
    "    f\"{col1}\": {\n",
    "        \"stats\": stats\n",
    "    }\n",
    "    for col1 in target_feature_cols\n",
    "}\n",
    "\n",
    "all_new_cols = []\n",
    "for col1, v in target_feature_sets.items():\n",
    "    new_cols = [f\"{col1}_{target}_{stat}\" for stat in stats]\n",
    "    target_feature_sets[col1]['new_cols'] = new_cols\n",
    "    all_new_cols.extend(new_cols)\n",
    "\n",
    "feature_sets = {\n",
    "    'material': {\n",
    "        'col2_list': ['is_waterproof', 'laptop_compartment', 'style', 'size'],\n",
    "        'stats': stats,\n",
    "    },\n",
    "}\n",
    "for col1, v in feature_sets.items():\n",
    "    col2_list = v['col2_list']\n",
    "    new_cols = [f\"{col1}_{col2}_{stat}\" for col2 in col2_list for stat in stats]\n",
    "    feature_sets[col1]['new_cols'] = new_cols\n",
    "    all_new_cols.extend(new_cols)\n",
    "\n",
    "features = [\n",
    "    'weight_capacity', 'compartments', 'laptop_compartment', 'is_waterproof', 'brand', 'color', 'size', 'material', 'style' \n",
    "] + all_new_cols + orig_df_columns\n",
    "\n",
    "print(f\"Using {len(features)} features.\")\n",
    "\n",
    "data_splits = []\n",
    "\n",
    "kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "for i, (train_idx, valid_idx) in enumerate(kf.split(train_df), 1):\n",
    "    train_fold = train_df.loc[train_idx].reset_index(drop=True)\n",
    "    valid_fold = train_df.loc[valid_idx].reset_index(drop=True)\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Inner K-Fold (for partial target encoding)\n",
    "    # -----------------------------------------------------\n",
    "    kf_inner = KFold(n_splits=inner_folds, shuffle=True, random_state=42)\n",
    "    for j, (inner_train_idx, inner_valid_idx) in enumerate(kf_inner.split(train_fold)):\n",
    "        inner_train = train_fold.loc[inner_train_idx].copy()\n",
    "        for col1, v in target_feature_sets.items():\n",
    "            stats = v['stats']\n",
    "            new_cols = v['new_cols']\n",
    "            agg_stats = inner_train.groupby(col1)[target].agg(stats)\n",
    "            for stat, new_col in zip(stats, new_cols):\n",
    "                train_fold.loc[inner_valid_idx, new_col] = train_fold.loc[inner_valid_idx, col1].map(agg_stats[stat])\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Outer K-Fold Add Feature Sets\n",
    "    # -----------------------------------------------------\n",
    "    for col1, v in target_feature_sets.items():\n",
    "        stats = v['stats']\n",
    "        new_cols = v['new_cols']\n",
    "        agg_stats = inner_train.groupby(col1)[target].agg(stats)\n",
    "        for stat, new_col in zip(stats, new_cols):\n",
    "            valid_fold.loc[:, new_col] = valid_fold.loc[:, col1].map(agg_stats[stat])\n",
    "\n",
    "    for col1, v in feature_sets.items():\n",
    "        stats = v['stats']\n",
    "        col2_list = v['col2_list']\n",
    "        new_cols = v['new_cols']\n",
    "        for k, col2 in enumerate(col2_list):\n",
    "            agg_stats = inner_train.groupby(col1)[col2].agg(stats)\n",
    "            for stat, new_col in zip(stats, new_cols[k*len(stats):(k+1)*len(stats)]):\n",
    "                train_fold.loc[:, new_col] = train_fold.loc[:, col1].map(agg_stats[stat])\n",
    "                valid_fold.loc[:, new_col] = valid_fold.loc[:, col1].map(agg_stats[stat])\n",
    "\n",
    "    train_fold[['compartments', 'laptop_compartment', 'is_waterproof', 'brand', 'color', 'size', 'material', 'style']] = \\\n",
    "        train_fold[['compartments', 'laptop_compartment', 'is_waterproof', 'brand', 'color', 'size', 'material', 'style']].astype('category') \n",
    "    \n",
    "    valid_fold[['compartments', 'laptop_compartment', 'is_waterproof', 'brand', 'color', 'size', 'material', 'style']] = \\\n",
    "        valid_fold[['compartments', 'laptop_compartment', 'is_waterproof', 'brand', 'color', 'size', 'material', 'style']].astype('category') \n",
    "\n",
    "    data_splits.append((train_fold, valid_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train_fold, valid_fold) in enumerate(data_splits, 1):\n",
    "    # Convert object columns to categorical in train fold\n",
    "    object_cols = train_fold.select_dtypes(include=['object']).columns\n",
    "    train_fold[object_cols] = train_fold[object_cols].astype('category')\n",
    "    valid_fold[object_cols] = valid_fold[object_cols].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'random_state': 42,\n",
    "        'verbose': -1,  # -1: Fatal, 0: Warning, 1: Info, 2: Debug\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'force_row_wise': True,\n",
    "        # 'early_stopping_rounds': 50, # the {n}th accuracy on the validation set does not improve, stop training\n",
    "        'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 50, 1000),\n",
    "\n",
    "        # bagging_fraction is like feature_fraction, but randomly selects data without resampling\n",
    "        # bagging_freq must be non-zero to enable bagging\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "\n",
    "        # default = 10.0,  used for the categorical features\n",
    "        'cat_l2':  trial.suggest_float('cat_l2', 0.01, 100),\n",
    "\n",
    "        # if set to true, when evaluating node splits LightGBM will check only one randomly-chosen threshold for each feature\n",
    "        'extra_trees': trial.suggest_categorical(\"extra_trees\", [True, False]),\n",
    "\n",
    "        # subset of features on each iteration (tree) to select\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        # colsample_bytree is ignored when feature_fraction is set\n",
    "        # 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.25, 0.35),\n",
    "\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "\n",
    "        # max number of bins that feature values will be bucketed in\n",
    "        'max_bin': trial.suggest_int('max_bin', 2, 20000),\n",
    "\n",
    "        # <= 0 means no limit. Used to deal with over-fitting when data is small. Tree still grows leaf-wise. \n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 2000),  \n",
    "\n",
    "        # Very important to prevent over-fitting. Setting it to hundreds or thousands is enough for a large dataset.\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 100),\n",
    "        'min_split_gain': 0.5,\n",
    "        \n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 20000),\n",
    "\n",
    "        # main parameter to control the complexity of the tree model. Should be smaller than 2^max_depth\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "\n",
    "        # subsample is ignored when bagging_fraction is set\n",
    "        # 'subsample': trial.suggest_float('subsample', 0.2, 0.25),\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "    rmse_list = []\n",
    "    for i, (train_fold, valid_fold) in enumerate(data_splits, 1):\n",
    "\n",
    "        train_data = lgb.Dataset(train_fold[features], label=train_fold[target])\n",
    "        valid_data = lgb.Dataset(valid_fold[features], label=valid_fold[target], reference=train_data)\n",
    "      \n",
    "        model = lgb.train(\n",
    "            params=params,\n",
    "            train_set=train_data,\n",
    "            valid_sets=[train_data, valid_data],\n",
    "            valid_names=['train_0', 'valid_0'],\n",
    "            callbacks=[\n",
    "                LightGBMPruningCallback(trial, \"rmse\", valid_name=\"valid_0\"),\n",
    "                lgb.log_evaluation(-1)                   # Suppress training logs\n",
    "            ]\n",
    "        )\n",
    "        y_pred = model.predict(valid_fold[features], num_iteration=model.best_iteration)\n",
    "        rmse = root_mean_squared_error(valid_fold[target], y_pred)\n",
    "        rmse_list.append(rmse)\n",
    "\n",
    "    return np.mean(rmse_list)\n",
    "\n",
    "study = optuna.create_study(\n",
    "        storage=f\"sqlite:///..//optuna//{model_str}db.sqlite3\",\n",
    "        study_name=model_str + study_name + datetime.now().strftime(\"%Y-%m-%d_%H-%M\"),\n",
    "        direction=\"minimize\"\n",
    ")\n",
    "study.optimize(objective, n_trials=1000)\n",
    "\n",
    "print(\"\\n=========================\")\n",
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial:\", study.best_trial.number)\n",
    "print(\"Best value (RMSE):\", study.best_trial.value)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "best_params = study.best_trial.params\n",
    "best_params[\"random_state\"] = 42\n",
    "best_params[\"verbose\"] = 0\n",
    "best_params[\"metric\"] = \"rmse\"\n",
    "best_params[\"force_row_wise\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train_fold, valid_fold) in enumerate(data_splits, 1):\n",
    "\n",
    "    train_data = lgb.Dataset(train_fold[features], label=train_fold[target])\n",
    "    valid_data = lgb.Dataset(valid_fold[features], label=valid_fold[target], reference=train_data)\n",
    "    break\n",
    "\n",
    "model = lgb.train(params=best_params, train_set=train_data, valid_sets=[train_data, valid_data])\n",
    "\n",
    "lgb.plot_importance(model, importance_type=\"gain\", figsize=(7,6), title=\"LightGBM Feature Importance (Gain)\")\n",
    "plt.show()\n",
    "\n",
    "lgb.plot_importance(model, importance_type=\"split\", figsize=(7,6), title=\"LightGBM Feature Importance (Split)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_split = model.feature_importance(importance_type='split')\n",
    "feature_importance_gain = model.feature_importance(importance_type='gain')\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Split Importance': feature_importance_split,\n",
    "    'Gain Importance': feature_importance_gain\n",
    "}).sort_values(by='Gain Importance', ascending=False)\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_lower = importance_df['Split Importance'].quantile(.5), \n",
    "gain_lower = importance_df['Gain Importance'].quantile(.5)\n",
    "\n",
    "# Features to remove based on conditions\n",
    "features_to_remove = importance_df[\n",
    "    (importance_df['Split Importance'] < split_lower) & \n",
    "    (importance_df['Gain Importance'] < gain_lower)\n",
    "]['Feature'].to_list()\n",
    "\n",
    "# Remove them from the 'features' list\n",
    "filtered_features = [feature for feature in features if feature not in features_to_remove]\n",
    "\n",
    "# Display the updated features list\n",
    "print(len(filtered_features), \"of\", len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'random_state': 42,\n",
    "        'verbose': -1,  # -1: Fatal, 0: Warning, 1: Info, 2: Debug\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'force_row_wise': True,\n",
    "        # 'early_stopping_rounds': 50, # the {n}th accuracy on the validation set does not improve, stop training\n",
    "        'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 50, 1000),\n",
    "\n",
    "        # bagging_fraction is like feature_fraction, but randomly selects data without resampling\n",
    "        # bagging_freq must be non-zero to enable bagging\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "\n",
    "        # default = 10.0,  used for the categorical features\n",
    "        'cat_l2':  trial.suggest_float('cat_l2', 0.01, 100),\n",
    "\n",
    "        # if set to true, when evaluating node splits LightGBM will check only one randomly-chosen threshold for each feature\n",
    "        'extra_trees': trial.suggest_categorical(\"extra_trees\", [True, False]),\n",
    "\n",
    "        # subset of features on each iteration (tree) to select\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        # colsample_bytree is ignored when feature_fraction is set\n",
    "        # 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.25, 0.35),\n",
    "\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "\n",
    "        # max number of bins that feature values will be bucketed in\n",
    "        'max_bin': trial.suggest_int('max_bin', 2, 20000),\n",
    "\n",
    "        # <= 0 means no limit. Used to deal with over-fitting when data is small. Tree still grows leaf-wise. \n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 2000),  \n",
    "\n",
    "        # Very important to prevent over-fitting. Setting it to hundreds or thousands is enough for a large dataset.\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 100),\n",
    "        'min_split_gain': 0.5,\n",
    "        \n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 20000),\n",
    "\n",
    "        # main parameter to control the complexity of the tree model. Should be smaller than 2^max_depth\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "\n",
    "        # subsample is ignored when bagging_fraction is set\n",
    "        # 'subsample': trial.suggest_float('subsample', 0.2, 0.25),\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "    rmse_list = []\n",
    "    for i, (train_fold, valid_fold) in enumerate(data_splits, 1):\n",
    "\n",
    "        train_data = lgb.Dataset(train_fold[filtered_features], label=train_fold[target])\n",
    "        valid_data = lgb.Dataset(valid_fold[filtered_features], label=valid_fold[target], reference=train_data)\n",
    "      \n",
    "        model = lgb.train(\n",
    "            params=params,\n",
    "            train_set=train_data,\n",
    "            valid_sets=[train_data, valid_data],\n",
    "            valid_names=['train_0', 'valid_0'],\n",
    "            callbacks=[\n",
    "                LightGBMPruningCallback(trial, \"rmse\", valid_name=\"valid_0\"),\n",
    "                lgb.log_evaluation(-1)                   # Suppress training logs\n",
    "            ]\n",
    "        )\n",
    "        y_pred = model.predict(valid_fold[filtered_features], num_iteration=model.best_iteration)\n",
    "        rmse = root_mean_squared_error(valid_fold[target], y_pred)\n",
    "        rmse_list.append(rmse)\n",
    "\n",
    "    return np.mean(rmse_list)\n",
    "\n",
    "study = optuna.create_study(\n",
    "        storage=f\"sqlite:///..//optuna//{model_str}db.sqlite3\",\n",
    "        study_name=model_str + study_name + datetime.now().strftime(\"%Y-%m-%d_%H-%M\"),\n",
    "        direction=\"minimize\"\n",
    ")\n",
    "study.optimize(objective, n_trials=1000)\n",
    "\n",
    "print(\"\\n=========================\")\n",
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial:\", study.best_trial.number)\n",
    "print(\"Best value (RMSE):\", study.best_trial.value)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "best_params = study.best_trial.params\n",
    "best_params[\"random_state\"] = 42\n",
    "best_params[\"verbose\"] = 0\n",
    "best_params[\"metric\"] = \"rmse\"\n",
    "best_params[\"force_row_wise\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train_fold, valid_fold) in enumerate(data_splits, 1):\n",
    "\n",
    "    train_data = lgb.Dataset(train_fold[filtered_features], label=train_fold[target])\n",
    "    valid_data = lgb.Dataset(valid_fold[filtered_features], label=valid_fold[target], reference=train_data)\n",
    "    break\n",
    "\n",
    "model = lgb.train(params=best_params, train_set=train_data, valid_sets=[train_data, valid_data])\n",
    "\n",
    "lgb.plot_importance(model, importance_type=\"gain\", figsize=(7,6), title=\"LightGBM Feature Importance (Gain)\")\n",
    "plt.show()\n",
    "\n",
    "lgb.plot_importance(model, importance_type=\"split\", figsize=(7,6), title=\"LightGBM Feature Importance (Split)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
