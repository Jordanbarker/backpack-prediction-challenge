{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from backpack_predictor import prepare_data, target_encoding\n",
    "from backpack_predictor.features import target, baseline_features, feature_list, cat_cols\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, chisquare, kruskal, ks_2samp, chi2_contingency\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "test_df = pd.read_csv(r'..//data//test.csv')\n",
    "train_df = pd.read_csv(r'..//data//train.csv')\n",
    "train_extra_df = pd.read_csv(r'..//data//training_extra.csv')\n",
    "train_df = pd.concat([train_df, train_extra_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Apply function to train and test datasets\n",
    "train_df = prepare_data(train_df, is_train=True)\n",
    "test_df = prepare_data(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250211_231009\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.10\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000\n",
      "CPU Count:          10\n",
      "Memory Avail:       23.25 GB / 64.00 GB (36.3%)\n",
      "Disk Space Avail:   439.58 GB / 926.35 GB (47.5%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (3994318 samples, 179.75 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250211_231009\"\n",
      "Train Data Rows:    3994318\n",
      "Train Data Columns: 9\n",
      "Label Column:       price\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (150.0, 15.0, 81.36217, 38.93868)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    24057.13 MB\n",
      "\tTrain Data (Original)  Memory Usage: 140.94 MB (0.6% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 5 | ['brand', 'material', 'compartments', 'style', 'color']\n",
      "\t\t('float', [])    : 2 | ['size', 'weight_capacity']\n",
      "\t\t('int', [])      : 2 | ['laptop_compartment', 'is_waterproof']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 5 | ['brand', 'material', 'compartments', 'style', 'color']\n",
      "\t\t('float', [])    : 2 | ['size', 'weight_capacity']\n",
      "\t\t('int', [])      : 2 | ['laptop_compartment', 'is_waterproof']\n",
      "\t3.1s = Fit runtime\n",
      "\t9 features in original data used to generate 9 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 140.95 MB (0.6% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.41s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 3954374, Val Rows: 39944\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-42.3661\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.55s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-45.1258\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.38s\t = Training   runtime\n",
      "\t0.54s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t-38.9346\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.19s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t-38.9231\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.58s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-38.9146\t = Validation score   (-root_mean_squared_error)\n",
      "\t811.06s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-38.9189\t = Validation score   (-root_mean_squared_error)\n",
      "\t5259.87s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-38.9286\t = Validation score   (-root_mean_squared_error)\n",
      "\t270.43s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 1: early stopping\n",
      "\t-38.946\t = Validation score   (-root_mean_squared_error)\n",
      "\t1840.9s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-38.9245\t = Validation score   (-root_mean_squared_error)\n",
      "\t49.94s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-38.9568\t = Validation score   (-root_mean_squared_error)\n",
      "\t1818.06s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t-38.9218\t = Validation score   (-root_mean_squared_error)\n",
      "\t50.65s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'RandomForestMSE': 0.529, 'CatBoost': 0.353, 'KNeighborsUnif': 0.059, 'NeuralNetTorch': 0.059}\n",
      "\t-38.8918\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 10198.56s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 38752.5 rows/s (39944 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250211_231009\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label=target, \n",
    "    problem_type='regression', \n",
    "    eval_metric='root_mean_squared_error'\n",
    ").fit(\n",
    "    train_df, \n",
    "    presets='best', # https://auto.gluon.ai/dev/tutorials/tabular/tabular-essentials.html#presets\n",
    "    # presets='medium_quality', \n",
    "    # time_limit=60 * 60 # 1 hour\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250212_152135\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.10\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000\n",
      "CPU Count:          10\n",
      "Memory Avail:       21.15 GB / 64.00 GB (33.0%)\n",
      "Disk Space Avail:   437.33 GB / 926.35 GB (47.2%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (3994318 samples, 179.75 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250212_152135\"\n",
      "Train Data Rows:    3994318\n",
      "Train Data Columns: 9\n",
      "Label Column:       price\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    21769.96 MB\n",
      "\tTrain Data (Original)  Memory Usage: 140.94 MB (0.6% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 5 | ['brand', 'material', 'compartments', 'style', 'color']\n",
      "\t\t('float', [])    : 2 | ['size', 'weight_capacity']\n",
      "\t\t('int', [])      : 2 | ['laptop_compartment', 'is_waterproof']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 5 | ['brand', 'material', 'compartments', 'style', 'color']\n",
      "\t\t('float', [])    : 2 | ['size', 'weight_capacity']\n",
      "\t\t('int', [])      : 2 | ['laptop_compartment', 'is_waterproof']\n",
      "\t2.9s = Fit runtime\n",
      "\t9 features in original data used to generate 9 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 140.95 MB (0.6% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 3954374, Val Rows: 39944\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}, {'learning_rate': 0.04748156996966733, 'num_leaves': 256, 'max_bin': 69779, 'lambda_l1': 0.0021556304294933115, 'lambda_l2': 0.05627364738597358, 'feature_fraction': 0.8, 'min_data_in_leaf': 24, 'ag_args': {'name_suffix': 'Custom1', 'priority': 100}}],\n",
      "}\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMCustom1 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 38.8651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-38.8638\t = Validation score   (-root_mean_squared_error)\n",
      "\t231.63s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t-38.9231\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.7s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBMCustom1': 1.0}\n",
      "\t-38.8638\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 257.45s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 88295.1 rows/s (39944 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250212_152135\")\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    'GBM': [\n",
    "        {},  # default LightGBM\n",
    "        # {'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}},  # LightGBMXT\n",
    "        {\n",
    "            'learning_rate': 0.04748156996966733,\n",
    "            'num_leaves': 256,\n",
    "            'max_bin': 69779,\n",
    "            'lambda_l1': 0.0021556304294933115,\n",
    "            'lambda_l2': 0.05627364738597358,\n",
    "            'feature_fraction': 0.8,\n",
    "            'min_data_in_leaf': 24,\n",
    "            'ag_args': {\n",
    "                'name_suffix': 'Custom1',\n",
    "                'priority': 100,  # High priority so it's sure to train\n",
    "            }\n",
    "        },\n",
    "        # You can keep adding more variants\n",
    "    ],\n",
    "    # You can also define XGB, CAT, etc. here if you want.\n",
    "}\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label=target, \n",
    "    problem_type='regression', \n",
    "    eval_metric='root_mean_squared_error'\n",
    ").fit(\n",
    "    train_df, \n",
    "    presets='medium_quality',  # or 'best_quality'\n",
    "    # num_stack_levels=2,  # enable multi-layer stacking\n",
    "    # num_bag_folds=5,     # or 10, the higher the better (usually)\n",
    "    hyperparameters=hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# te = TargetEncoder(target_type=\"continuous\", smooth=20)\n",
    "# train_te_all = te.fit_transform(train_fold[candidate_cols], train_fold[target])\n",
    "# val_te_all = te.transform(val_fold[candidate_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['brand', 'material', 'size', 'compartments', 'laptop_compartment',\n",
       "       'is_waterproof', 'style', 'color', 'weight_capacity', 'price',\n",
       "       'brand_material_weight_combined', 'brand_size_weight_combined',\n",
       "       'brand_compartments_weight_combined', 'brand_style_weight_combined',\n",
       "       'brand_color_weight_combined',\n",
       "       'brand_laptop_compartment_weight_combined',\n",
       "       'brand_is_waterproof_weight_combined', 'material_size_weight_combined',\n",
       "       'material_compartments_weight_combined',\n",
       "       'material_style_weight_combined', 'material_color_weight_combined',\n",
       "       'material_laptop_compartment_weight_combined',\n",
       "       'material_is_waterproof_weight_combined',\n",
       "       'size_compartments_weight_combined', 'size_style_weight_combined',\n",
       "       'size_color_weight_combined', 'size_laptop_compartment_weight_combined',\n",
       "       'size_is_waterproof_weight_combined',\n",
       "       'compartments_style_weight_combined',\n",
       "       'compartments_color_weight_combined',\n",
       "       'compartments_laptop_compartment_weight_combined',\n",
       "       'compartments_is_waterproof_weight_combined',\n",
       "       'style_color_weight_combined',\n",
       "       'style_laptop_compartment_weight_combined',\n",
       "       'style_is_waterproof_weight_combined',\n",
       "       'color_laptop_compartment_weight_combined',\n",
       "       'color_is_waterproof_weight_combined',\n",
       "       'laptop_compartment_is_waterproof_weight_combined'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Ensure all categorical columns are of type 'category' and create code columns.\n",
    "for col in cat_cols:\n",
    "    if train_df[col].dtype != 'category':\n",
    "        train_df[col] = train_df[col].astype('category')\n",
    "    train_df[f\"{col}_code\"] = train_df[col].cat.codes\n",
    "\n",
    "# For each pair of categorical columns, combine their codes with weight_capacity.\n",
    "# The function below is applied in a vectorized way using np.where.\n",
    "for col1, col2 in combinations(cat_cols, 2):\n",
    "    code1 = train_df[f\"{col1}_code\"]\n",
    "    code2 = train_df[f\"{col2}_code\"]\n",
    "    weight = train_df[\"weight_capacity\"]\n",
    "    \n",
    "    # Create a boolean mask where either code is -1.\n",
    "    mask = (code1 == -1) | (code2 == -1)\n",
    "    \n",
    "    # If either code is -1, we compute -1*weight - 1, else we combine them.\n",
    "    # Adjust multipliers (here 10000 and 100) as needed for your data scale.\n",
    "    combined = np.where(mask, \n",
    "                        -1 * weight - 1, \n",
    "                        code1 * 10000 + code2 * 100 + weight)\n",
    "    \n",
    "    new_col_name = f\"{col1}_{col2}_weight_combined\"\n",
    "    train_df[new_col_name] = combined\n",
    "\n",
    "temp_code_cols = [f\"{col}_code\" for col in cat_cols]\n",
    "train_df.drop(columns=temp_code_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['brand', 'material', 'size', 'compartments', 'laptop_compartment',\n",
       "       'is_waterproof', 'style', 'color', 'weight_capacity', 'price',\n",
       "       'brand_material_weight_combined', 'brand_size_weight_combined',\n",
       "       'brand_compartments_weight_combined', 'brand_style_weight_combined',\n",
       "       'brand_color_weight_combined',\n",
       "       'brand_laptop_compartment_weight_combined',\n",
       "       'brand_is_waterproof_weight_combined', 'material_size_weight_combined',\n",
       "       'material_compartments_weight_combined',\n",
       "       'material_style_weight_combined', 'material_color_weight_combined',\n",
       "       'material_laptop_compartment_weight_combined',\n",
       "       'material_is_waterproof_weight_combined',\n",
       "       'size_compartments_weight_combined', 'size_style_weight_combined',\n",
       "       'size_color_weight_combined', 'size_laptop_compartment_weight_combined',\n",
       "       'size_is_waterproof_weight_combined',\n",
       "       'compartments_style_weight_combined',\n",
       "       'compartments_color_weight_combined',\n",
       "       'compartments_laptop_compartment_weight_combined',\n",
       "       'compartments_is_waterproof_weight_combined',\n",
       "       'style_color_weight_combined',\n",
       "       'style_laptop_compartment_weight_combined',\n",
       "       'style_is_waterproof_weight_combined',\n",
       "       'color_laptop_compartment_weight_combined',\n",
       "       'color_is_waterproof_weight_combined',\n",
       "       'laptop_compartment_is_waterproof_weight_combined'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +11m11s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250212_193222\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preset alias specified: 'best' maps to 'best_quality'.\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.10\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\n",
      "CPU Count:          10\n",
      "Memory Avail:       34.21 GB / 64.00 GB (53.5%)\n",
      "Disk Space Avail:   462.20 GB / 926.35 GB (49.9%)\n",
      "===================================================\n",
      "Presets specified: ['best']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 5400s of the 21600s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-02-12 13:32:24,043\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250212_193222/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Beginning AutoGluon training ... Time limit = 5398s\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m AutoGluon will save models to \"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250212_193222/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Train Data Rows:    3550504\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Train Data Columns: 21\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Label Column:       price\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tAvailable Memory:                    34307.66 MB\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tTrain Data (Original)  Memory Usage: 379.24 MB (1.1% of available memory)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t\t('category', []) :  8 | ['brand', 'material', 'size', 'compartments', 'laptop_compartment', ...]\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t\t('float', [])    : 13 | ['weight_capacity', 'brand_color_weight_combined', 'brand_is_waterproof_weight_combined', 'brand_material_weight_combined', 'brand_size_weight_combined', ...]\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t\t('category', []) :  8 | ['brand', 'material', 'size', 'compartments', 'laptop_compartment', ...]\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t\t('float', [])    : 13 | ['weight_capacity', 'brand_color_weight_combined', 'brand_is_waterproof_weight_combined', 'brand_material_weight_combined', 'brand_size_weight_combined', ...]\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t7.0s = Fit runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t21 features in original data used to generate 21 features in processed data.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tTrain Data (Processed) Memory Usage: 379.24 MB (1.1% of available memory)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Data preprocessing and feature engineering runtime = 7.28s ...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 3592.96s of the 5390.78s of remaining time.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t-42.601\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t8.44s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t5.42s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 3575.77s of the 5373.59s of remaining time.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t-45.9746\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t8.59s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t5.51s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3558.90s of the 5356.73s of remaining time.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.79%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=4517)\u001b[0m [1000]\tvalid_set's rmse: 38.8623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t-38.8848\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t137.14s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t106.79s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 3401.13s of the 5198.95s of remaining time.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.37%)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t-38.8758\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t75.64s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t49.96s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 3311.28s of the 5109.10s of remaining time.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 143 due to low time. Expected time usage reduced from 6922.6s -> 3306.8s...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t-38.9351\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t1876.28s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t39.54s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 1392.87s of the 3190.69s of remaining time.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.03%)\n",
      "\u001b[36m(_ray_fit pid=5955)\u001b[0m \tRan out of time, early stopping on iteration 246.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t-38.8873\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t1108.22s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t1.16s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 277.41s of the 2075.24s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=5952)\u001b[0m \tRan out of time, early stopping on iteration 246.\u001b[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 108 due to low time. Expected time usage reduced from 751.5s -> 272.5s...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t-38.9023\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t173.75s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t22.99s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 77.83s of the 1875.65s of remaining time.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.96%)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L1.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 46.00s of the 1843.82s of remaining time.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.79%)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t-38.9331\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t196.2s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t6.69s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1639.83s of remaining time.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L1': 0.824, 'RandomForestMSE_BAG_L1': 0.118, 'LightGBMXT_BAG_L1': 0.059}\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t-38.8747\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t0.77s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1638.92s of the 1638.75s of remaining time.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.62%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=7177)\u001b[0m [1000]\tvalid_set's rmse: 38.846\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7173)\u001b[0m [1000]\tvalid_set's rmse: 38.8716\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7177)\u001b[0m [2000]\tvalid_set's rmse: 38.8435\n",
      "\u001b[36m(_ray_fit pid=7172)\u001b[0m [2000]\tvalid_set's rmse: 38.839\n",
      "\u001b[36m(_ray_fit pid=7173)\u001b[0m [2000]\tvalid_set's rmse: 38.8705\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7177)\u001b[0m [3000]\tvalid_set's rmse: 38.8419\n",
      "\u001b[36m(_ray_fit pid=7172)\u001b[0m [3000]\tvalid_set's rmse: 38.8366\n",
      "\u001b[36m(_ray_fit pid=7173)\u001b[0m [3000]\tvalid_set's rmse: 38.8692\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7177)\u001b[0m [4000]\tvalid_set's rmse: 38.8413\n",
      "\u001b[36m(_ray_fit pid=7174)\u001b[0m [4000]\tvalid_set's rmse: 38.8798\n",
      "\u001b[36m(_ray_fit pid=7176)\u001b[0m [4000]\tvalid_set's rmse: 38.8545\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7177)\u001b[0m [5000]\tvalid_set's rmse: 38.8398\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7176)\u001b[0m [5000]\tvalid_set's rmse: 38.8538\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7177)\u001b[0m [6000]\tvalid_set's rmse: 38.8388\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7176)\u001b[0m [6000]\tvalid_set's rmse: 38.8528\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7177)\u001b[0m [7000]\tvalid_set's rmse: 38.8389\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7172)\u001b[0m [7000]\tvalid_set's rmse: 38.833\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7177)\u001b[0m [8000]\tvalid_set's rmse: 38.8377\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7172)\u001b[0m [8000]\tvalid_set's rmse: 38.8318\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7177)\u001b[0m [9000]\tvalid_set's rmse: 38.8356\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7172)\u001b[0m [9000]\tvalid_set's rmse: 38.8319\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7177)\u001b[0m [10000]\tvalid_set's rmse: 38.836\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7172)\u001b[0m [10000]\tvalid_set's rmse: 38.8309\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t-38.8616\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t961.14s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t2917.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 106.03s of the 105.86s of remaining time.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.74%)\n",
      "\u001b[36m(_ray_fit pid=7740)\u001b[0m \tRan out of time, early stopping on iteration 714. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=7740)\u001b[0m \t[714]\tvalid_set's rmse: 38.8809\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t-38.8646\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t80.44s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t62.84s\t = Validation runtime\n",
      "\u001b[36m(_ray_fit pid=7743)\u001b[0m \tRan out of time, early stopping on iteration 710. Best iteration is:\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7743)\u001b[0m \t[708]\tvalid_set's rmse: 38.8418\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 8.52s of remaining time.\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.529, 'LightGBM_BAG_L2': 0.471}\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t-38.8533\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t0.87s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m AutoGluon training complete, total runtime = 5390.69s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 140.7 rows/s (443813 batch size)\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250212_193222/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=4450)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0          LightGBM_BAG_L2     -38.861477 -38.864585  root_mean_squared_error       39.385982     300.915218  3664.706642                 8.188591               62.844734          80.441129            2       True         11\n",
      "1      WeightedEnsemble_L2     -38.863978 -38.874666  root_mean_squared_error       21.492587     196.319961  2089.825571                 0.004399                0.032275           0.769435            2       True          9\n",
      "2          LightGBM_BAG_L1     -38.864143 -38.875841  root_mean_squared_error        6.834707      49.958093    75.635008                 6.834707               49.958093          75.635008            1       True          4\n",
      "3      WeightedEnsemble_L3     -38.867018 -38.853301  root_mean_squared_error      240.751914    3218.017179  4626.717985                 0.003871                0.029558           0.872888            3       True         12\n",
      "4        LightGBMXT_BAG_L1     -38.875522 -38.884841  root_mean_squared_error       13.311909     106.792826   137.144163                13.311909              106.792826         137.144163            1       True          3\n",
      "5        LightGBMXT_BAG_L2     -38.878483 -38.861573  root_mean_squared_error      232.559452    3155.142888  4545.403968               201.362061             2917.072403         961.138455            2       True         10\n",
      "6          CatBoost_BAG_L1     -38.879192 -38.887292  root_mean_squared_error        1.037663       1.164629  1108.215543                 1.037663                1.164629        1108.215543            1       True          6\n",
      "7     ExtraTreesMSE_BAG_L1     -38.888707 -38.902303  root_mean_squared_error        0.701439      22.994729   173.752750                 0.701439               22.994729         173.752750            1       True          7\n",
      "8   RandomForestMSE_BAG_L1     -38.916105 -38.935148  root_mean_squared_error        1.341572      39.536767  1876.276965                 1.341572               39.536767        1876.276965            1       True          5\n",
      "9           XGBoost_BAG_L1     -38.924584 -38.933054  root_mean_squared_error        6.350020       6.690362   196.204309                 6.350020                6.690362         196.204309            1       True          8\n",
      "10   KNeighborsUnif_BAG_L1     -42.536608 -42.600990  root_mean_squared_error        0.806498       5.419371     8.442551                 0.806498                5.419371           8.442551            1       True          1\n",
      "11   KNeighborsDist_BAG_L1     -45.932583 -45.974557  root_mean_squared_error        0.813583       5.513707     8.594224                 0.813583                5.513707           8.594224            1       True          2\n",
      "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
      "\t5635s\t = DyStack   runtime |\t15965s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=0.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
      "Beginning AutoGluon training ... Time limit = 15965s\n",
      "AutoGluon will save models to \"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250212_193222\"\n",
      "Train Data Rows:    3994318\n",
      "Train Data Columns: 21\n",
      "Label Column:       price\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    47120.70 MB\n",
      "\tTrain Data (Original)  Memory Usage: 426.64 MB (0.9% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  8 | ['brand', 'material', 'size', 'compartments', 'laptop_compartment', ...]\n",
      "\t\t('float', [])    : 13 | ['weight_capacity', 'brand_color_weight_combined', 'brand_is_waterproof_weight_combined', 'brand_material_weight_combined', 'brand_size_weight_combined', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  8 | ['brand', 'material', 'size', 'compartments', 'laptop_compartment', ...]\n",
      "\t\t('float', [])    : 13 | ['weight_capacity', 'brand_color_weight_combined', 'brand_is_waterproof_weight_combined', 'brand_material_weight_combined', 'brand_size_weight_combined', ...]\n",
      "\t8.1s = Fit runtime\n",
      "\t21 features in original data used to generate 21 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 426.64 MB (0.9% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 8.38s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 15956.41s of the 15956.40s of remaining time.\n",
      "\t-42.5914\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.97s\t = Training   runtime\n",
      "\t5.58s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 15933.92s of the 15933.91s of remaining time.\n",
      "\t-45.9965\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.09s\t = Training   runtime\n",
      "\t5.66s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 15912.12s of the 15912.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.48%)\n",
      "\t-38.8827\t = Validation score   (-root_mean_squared_error)\n",
      "\t182.15s\t = Training   runtime\n",
      "\t134.93s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 15703.65s of the 15703.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.45%)\n",
      "\t-38.8729\t = Validation score   (-root_mean_squared_error)\n",
      "\t86.46s\t = Training   runtime\n",
      "\t62.6s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 15601.71s of the 15601.70s of remaining time.\n",
      "\t-38.9156\t = Validation score   (-root_mean_squared_error)\n",
      "\t4474.05s\t = Training   runtime\n",
      "\t92.92s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 11031.51s of the 11031.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.39%)\n",
      "\t-38.8749\t = Validation score   (-root_mean_squared_error)\n",
      "\t8825.79s\t = Training   runtime\n",
      "\t4.08s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 2197.81s of the 2197.80s of remaining time.\n",
      "\t-38.8895\t = Validation score   (-root_mean_squared_error)\n",
      "\t524.79s\t = Training   runtime\n",
      "\t68.88s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1600.79s of the 1600.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.70%)\n",
      "\t-38.9008\t = Validation score   (-root_mean_squared_error)\n",
      "\t1194.21s\t = Training   runtime\n",
      "\t13.75s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 397.46s of the 397.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.48%)\n",
      "\t-38.9266\t = Validation score   (-root_mean_squared_error)\n",
      "\t463.95s\t = Training   runtime\n",
      "\t9.18s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 1595.64s of the -74.98s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.571, 'CatBoost_BAG_L1': 0.286, 'RandomForestMSE_BAG_L1': 0.143}\n",
      "\t-38.8707\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.93s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 16040.98s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6376.2 rows/s (499290 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250212_193222\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label=target, \n",
    "    problem_type='regression', \n",
    "    eval_metric='root_mean_squared_error'\n",
    ").fit(\n",
    "    train_df[[\n",
    "        'brand', 'material', 'size', 'compartments', 'laptop_compartment',\n",
    "        'is_waterproof', 'style', 'color', 'weight_capacity', 'price',\n",
    "        'brand_color_weight_combined',\n",
    "        'brand_is_waterproof_weight_combined',\n",
    "        'brand_material_weight_combined',\n",
    "        'brand_size_weight_combined',\n",
    "        'laptop_compartment_is_waterproof_weight_combined',\n",
    "        'material_color_weight_combined',\n",
    "        'material_is_waterproof_weight_combined',\n",
    "        'material_laptop_compartment_weight_combined',\n",
    "        'material_size_weight_combined',\n",
    "        'material_style_weight_combined',\n",
    "        'size_style_weight_combined',\n",
    "        'style_color_weight_combined',\n",
    "    ]].copy(), \n",
    "    presets='best', # https://auto.gluon.ai/dev/tutorials/tabular/tabular-essentials.html#presets\n",
    "    time_limit=60 * 60 * 6 # 60 is 1 minute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brand_color_weight_combined',\n",
       " 'brand_is_waterproof_weight_combined',\n",
       " 'brand_material_weight_combined',\n",
       " 'brand_size_weight_combined',\n",
       " 'laptop_compartment_is_waterproof_weight_combined',\n",
       " 'material_color_weight_combined',\n",
       " 'material_is_waterproof_weight_combined',\n",
       " 'material_laptop_compartment_weight_combined',\n",
       " 'material_size_weight_combined',\n",
       " 'material_style_weight_combined',\n",
       " 'size_style_weight_combined',\n",
       " 'style_color_weight_combined'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_feature_list = [\n",
    "    ['brand_material_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['material_color_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['brand_color_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['material_size_weight_combined', 'style_color_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['brand_is_waterproof_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['material_is_waterproof_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['material_laptop_compartment_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['brand_size_weight_combined', 'material_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['material_size_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['brand_size_weight_combined', 'style_color_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "]\n",
    "f_list = []\n",
    "for f in three_feature_list:\n",
    "    for c in f:\n",
    "        f_list.append(c)\n",
    "set(f_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250212_154700\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.10\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000\n",
      "CPU Count:          10\n",
      "Memory Avail:       22.95 GB / 64.00 GB (35.9%)\n",
      "Disk Space Avail:   436.02 GB / 926.35 GB (47.1%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (3994318 samples, 990.59 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250212_154700\"\n",
      "Train Data Rows:    3994318\n",
      "Train Data Columns: 37\n",
      "Label Column:       price\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    23830.70 MB\n",
      "\tTrain Data (Original)  Memory Usage: 914.23 MB (3.8% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  8 | ['brand', 'material', 'size', 'compartments', 'laptop_compartment', ...]\n",
      "\t\t('float', [])    : 29 | ['weight_capacity', 'brand_material_weight_combined', 'brand_size_weight_combined', 'brand_compartments_weight_combined', 'brand_style_weight_combined', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  8 | ['brand', 'material', 'size', 'compartments', 'laptop_compartment', ...]\n",
      "\t\t('float', [])    : 29 | ['weight_capacity', 'brand_material_weight_combined', 'brand_size_weight_combined', 'brand_compartments_weight_combined', 'brand_style_weight_combined', ...]\n",
      "\t15.6s = Fit runtime\n",
      "\t37 features in original data used to generate 37 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 914.23 MB (3.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 16.31s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 3954374, Val Rows: 39944\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{'learning_rate': 0.04748156996966733, 'num_leaves': 256, 'max_bin': 69779, 'lambda_l1': 0.0021556304294933115, 'lambda_l2': 0.05627364738597358, 'feature_fraction': 0.8, 'min_data_in_leaf': 24, 'ag_args': {'name_suffix': '_3F_1'}, 'ag_args_fit': {'only_use_features': ['brand_material_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined']}}, {'learning_rate': 0.04748156996966733, 'num_leaves': 256, 'max_bin': 69779, 'lambda_l1': 0.0021556304294933115, 'lambda_l2': 0.05627364738597358, 'feature_fraction': 0.8, 'min_data_in_leaf': 24, 'ag_args': {'name_suffix': '_3F_2'}, 'ag_args_fit': {'only_use_features': ['material_color_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined']}}, {'learning_rate': 0.04748156996966733, 'num_leaves': 256, 'max_bin': 69779, 'lambda_l1': 0.0021556304294933115, 'lambda_l2': 0.05627364738597358, 'feature_fraction': 0.8, 'min_data_in_leaf': 24, 'ag_args': {'name_suffix': '_3F_3'}, 'ag_args_fit': {'only_use_features': ['brand_color_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined']}}, {'learning_rate': 0.04748156996966733, 'num_leaves': 256, 'max_bin': 69779, 'lambda_l1': 0.0021556304294933115, 'lambda_l2': 0.05627364738597358, 'feature_fraction': 0.8, 'min_data_in_leaf': 24, 'ag_args': {'name_suffix': '_3F_4'}, 'ag_args_fit': {'only_use_features': ['material_size_weight_combined', 'style_color_weight_combined', 'laptop_compartment_is_waterproof_weight_combined']}}, {'learning_rate': 0.04748156996966733, 'num_leaves': 256, 'max_bin': 69779, 'lambda_l1': 0.0021556304294933115, 'lambda_l2': 0.05627364738597358, 'feature_fraction': 0.8, 'min_data_in_leaf': 24, 'ag_args': {'name_suffix': '_3F_5'}, 'ag_args_fit': {'only_use_features': ['brand_is_waterproof_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined']}}, {'learning_rate': 0.04748156996966733, 'num_leaves': 256, 'max_bin': 69779, 'lambda_l1': 0.0021556304294933115, 'lambda_l2': 0.05627364738597358, 'feature_fraction': 0.8, 'min_data_in_leaf': 24, 'ag_args': {'name_suffix': '_3F_6'}, 'ag_args_fit': {'only_use_features': ['material_is_waterproof_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined']}}, {'learning_rate': 0.04748156996966733, 'num_leaves': 256, 'max_bin': 69779, 'lambda_l1': 0.0021556304294933115, 'lambda_l2': 0.05627364738597358, 'feature_fraction': 0.8, 'min_data_in_leaf': 24, 'ag_args': {'name_suffix': '_3F_7'}, 'ag_args_fit': {'only_use_features': ['material_laptop_compartment_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined']}}, {'learning_rate': 0.04748156996966733, 'num_leaves': 256, 'max_bin': 69779, 'lambda_l1': 0.0021556304294933115, 'lambda_l2': 0.05627364738597358, 'feature_fraction': 0.8, 'min_data_in_leaf': 24, 'ag_args': {'name_suffix': '_3F_8'}, 'ag_args_fit': {'only_use_features': ['brand_size_weight_combined', 'material_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined']}}, {'learning_rate': 0.04748156996966733, 'num_leaves': 256, 'max_bin': 69779, 'lambda_l1': 0.0021556304294933115, 'lambda_l2': 0.05627364738597358, 'feature_fraction': 0.8, 'min_data_in_leaf': 24, 'ag_args': {'name_suffix': '_3F_9'}, 'ag_args_fit': {'only_use_features': ['material_size_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined']}}, {'learning_rate': 0.04748156996966733, 'num_leaves': 256, 'max_bin': 69779, 'lambda_l1': 0.0021556304294933115, 'lambda_l2': 0.05627364738597358, 'feature_fraction': 0.8, 'min_data_in_leaf': 24, 'ag_args': {'name_suffix': '_3F_10'}, 'ag_args_fit': {'only_use_features': ['brand_size_weight_combined', 'style_color_weight_combined', 'laptop_compartment_is_waterproof_weight_combined']}}],\n",
      "}\n",
      "Fitting 10 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_3F_1 ...\n",
      "\t-38.9006\t = Validation score   (-root_mean_squared_error)\n",
      "\t147.84s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBM_3F_2 ...\n",
      "\t-38.9006\t = Validation score   (-root_mean_squared_error)\n",
      "\t150.83s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBM_3F_3 ...\n",
      "\t-38.9006\t = Validation score   (-root_mean_squared_error)\n",
      "\t152.52s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBM_3F_4 ...\n",
      "\t-38.9006\t = Validation score   (-root_mean_squared_error)\n",
      "\t152.19s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBM_3F_5 ...\n",
      "\t-38.9006\t = Validation score   (-root_mean_squared_error)\n",
      "\t149.46s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBM_3F_6 ...\n",
      "\t-38.9006\t = Validation score   (-root_mean_squared_error)\n",
      "\t154.74s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBM_3F_7 ...\n",
      "\t-38.9006\t = Validation score   (-root_mean_squared_error)\n",
      "\t155.74s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBM_3F_8 ...\n",
      "\t-38.9006\t = Validation score   (-root_mean_squared_error)\n",
      "\t160.34s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_3F_9 ...\n",
      "\t-38.9006\t = Validation score   (-root_mean_squared_error)\n",
      "\t156.19s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBM_3F_10 ...\n",
      "\t-38.9006\t = Validation score   (-root_mean_squared_error)\n",
      "\t151.96s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBM_3F_6': 1.0}\n",
      "\t-38.9006\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1550.19s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 404169.8 rows/s (39944 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/jordanbarker/Documents/Kaggle/backpack-prediction-challenge/notebooks/AutogluonModels/ag-20250212_154700\")\n"
     ]
    }
   ],
   "source": [
    "three_feature_list = [\n",
    "    ['brand_material_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['material_color_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['brand_color_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['material_size_weight_combined', 'style_color_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['brand_is_waterproof_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['material_is_waterproof_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['material_laptop_compartment_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['brand_size_weight_combined', 'material_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['material_size_weight_combined', 'size_style_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "    ['brand_size_weight_combined', 'style_color_weight_combined', 'laptop_compartment_is_waterproof_weight_combined'],\n",
    "]\n",
    "\n",
    "shared_lgbm_params = {\n",
    "    'learning_rate': 0.04748156996966733,\n",
    "    'num_leaves': 256,\n",
    "    'max_bin': 69779,\n",
    "    'lambda_l1': 0.0021556304294933115,\n",
    "    'lambda_l2': 0.05627364738597358,\n",
    "    'feature_fraction': 0.8,\n",
    "    'min_data_in_leaf': 24,\n",
    "}\n",
    "\n",
    "# Build the hyperparameters dict for AutoGluon\n",
    "hyperparams = {'GBM': []}\n",
    "for i, feature_set in enumerate(three_feature_list):\n",
    "    model_name_suffix = f'_3F_{i+1}'\n",
    "    model_params = {\n",
    "        **shared_lgbm_params,\n",
    "        'ag_args': {\n",
    "            'name_suffix': model_name_suffix, \n",
    "        },\n",
    "        'ag_args_fit': {\n",
    "            'only_use_features': feature_set,\n",
    "        },\n",
    "    }\n",
    "    hyperparams['GBM'].append(model_params)\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label=target, \n",
    "    problem_type='regression', \n",
    "    eval_metric='root_mean_squared_error'\n",
    ").fit(\n",
    "    train_df, \n",
    "    presets='medium_quality',  # or 'best_quality'\n",
    "    # num_stack_levels=2,  # enable multi-layer stacking\n",
    "    # num_bag_folds=5,     # or 10, the higher the better (usually)\n",
    "    hyperparameters=hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
